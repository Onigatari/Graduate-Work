{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3bd3b4-219b-4c3b-a7ee-61d253092206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Input, Dense,Embedding\n",
    "from keras.models import Model,load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4dd2f02-e395-49ef-83a5-561264659141",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rus-oss.txt', encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8594834f-8c22-42f2-9c9d-824682476705",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data_list = data.split('\\n')\n",
    "uncleaned_data_list = uncleaned_data_list[:38695]\n",
    "\n",
    "rus_word = []\n",
    "oss_word = []\n",
    "\n",
    "for word in uncleaned_data_list:\n",
    "    rus_word.append(word.split('\\t')[0])\n",
    "    oss_word.append(word.split('\\t')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f26833-35c5-4c74-be15-a6aeee953077",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_data = pd.DataFrame(columns=['Russia','Ossetian'])\n",
    "language_data['Russia'] = rus_word\n",
    "language_data['Ossetian'] = oss_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b1b8388-5baf-46d9-82e5-f82e11c68096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to csv\n",
    "language_data.to_csv('language_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61839196-3044-4a75-9c9d-d31d92c56e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data from csv\n",
    "language_data = pd.read_csv('language_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74166bf2-1b9e-4c76-97cb-dcc10a6a832d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Russia</th>\n",
       "      <th>Ossetian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Чего ты смеёшься?</td>\n",
       "      <td>Цæуыл худыс?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Этот нож очень острый.</td>\n",
       "      <td>Ацы кард тынг цыргъ у.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>У кошки девять жизней.</td>\n",
       "      <td>Гæдыйæн фараст царды ис.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Сегодня облачно.</td>\n",
       "      <td>Абон у асæст.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Он был вождём своего племени 35 лет.</td>\n",
       "      <td>Уый йæ знæмы раздзог уыдис 35 азы дæргъы.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Russia  \\\n",
       "0                     Чего ты смеёшься?   \n",
       "1                Этот нож очень острый.   \n",
       "2                У кошки девять жизней.   \n",
       "3                      Сегодня облачно.   \n",
       "4  Он был вождём своего племени 35 лет.   \n",
       "\n",
       "                                    Ossetian  \n",
       "0                               Цæуыл худыс?  \n",
       "1                     Ацы кард тынг цыргъ у.  \n",
       "2                   Гæдыйæн фараст царды ис.  \n",
       "3                              Абон у асæст.  \n",
       "4  Уый йæ знæмы раздзог уыдис 35 азы дæргъы.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfaae43d-4b63-4b8e-9cfd-374bed9eaea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Russia</th>\n",
       "      <th>Ossetian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>Сегодня будет дождь?</td>\n",
       "      <td>Абон уардзæн?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>Какое сегодня число?</td>\n",
       "      <td>Абон кæцы бон у?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>Какой сегодня день?</td>\n",
       "      <td>Цы бон у абон?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>Вы говорите по-осетински?</td>\n",
       "      <td>Иронау дзурут?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>Вы кого-то ищете?</td>\n",
       "      <td>Искæй агурут?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Russia          Ossetian\n",
       "425       Сегодня будет дождь?     Абон уардзæн?\n",
       "426       Какое сегодня число?  Абон кæцы бон у?\n",
       "427        Какой сегодня день?    Цы бон у абон?\n",
       "428  Вы говорите по-осетински?    Иронау дзурут?\n",
       "429          Вы кого-то ищете?     Искæй агурут?"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d07c12e3-2ae9-4ff9-910d-157045ae2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_text = language_data['Russia'].values\n",
    "oss_text = language_data['Ossetian'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d4e6e5-6446-49c1-b5b3-2f06c0caf170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Чего ты смеёшься?', 'Цæуыл худыс?')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus_text[0], oss_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9becfba-23ea-4161-baf7-db337fff0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercasing the setences\n",
    "rus_text_ = [x.lower() for x in rus_text]\n",
    "oss_text_ = [x.lower() for x in oss_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a263f311-c1f2-4398-9cb7-25b657d87f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_text_ = [re.sub(\"'\",'',x) for x in rus_text_]\n",
    "oss_text_ = [re.sub(\"'\",'',x) for x in oss_text_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47301419-81a6-4873-b20f-238a0207d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove puntuation\n",
    "def remove_punc(text_list):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    removed_punc_text = []\n",
    "    for sent in text_list:\n",
    "        sentance = [w.translate(table) for w in sent.split(' ')]\n",
    "        removed_punc_text.append(' '.join(sentance))\n",
    "    return removed_punc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a57960b-aab3-4e15-b1d2-f5409561525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_text_ = remove_punc(rus_text_)\n",
    "oss_text_ = remove_punc(oss_text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5251ea54-15e2-48db-a9b8-95485cfd8ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the digits from russian sentances\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "removed_digits_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a1752b7-0ec5-4ed7-81ae-da8ae2f9cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in rus_text_:\n",
    "    sentance = [w.translate(remove_digits) for w in sent.split(' ')]\n",
    "    removed_digits_text.append(' '.join(sentance))\n",
    "    \n",
    "rus_text_ = removed_digits_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "430f9bf7-974c-4c5c-9f08-20f196c87fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_text_ = [x.strip() for x in rus_text_]\n",
    "oss_text_ = [x.strip() for x in oss_text_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b11ab261-579e-4796-9270-b7412d744761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rus_text_ = [\"<sos> \" + x + \" <eos>\" for x in rus_text_]\n",
    "oss_text_ = [\"start \" + x + \" end\" for x in oss_text_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31e666cc-a2c4-44fa-b2be-5fce7a707e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<sos> цæуыл худыс <eos>', 'чего ты смеёшься')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oss_text_[0], rus_text_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66aacda-d7c5-432e-b4ef-26894828d82b",
   "metadata": {},
   "source": [
    "# Data spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cd1a947-3809-44e3-9a95-f7beb88e9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rus_text_\n",
    "Y = oss_text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f162257-a1b4-4f95-8924-c0a8d8cfec23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387, 387, 43, 43)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.1)\n",
    "len(X_train),len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "831ccdc4-2ac5-47a6-bdf8-03d08ecd2652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('чего ты смеёшься', '<sos> цæуыл худыс <eos>')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], Y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a866d-44e2-47a3-8347-8f343cadafe6",
   "metadata": {},
   "source": [
    "## Data preparing for encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "979cfe58-63c5-4c4a-be81-23caf194cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for the word embedding\n",
    "def Max_length(data):\n",
    "    max_length_ = max([len(x.split(' ')) for x in data])\n",
    "    return max_length_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8a27121-00cd-48b0-93bb-d83113a79c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data\n",
    "max_length_rus = Max_length(X_train)\n",
    "max_lenght_oss = Max_length(y_train)\n",
    "\n",
    "#Test data\n",
    "max_length_rus_test = Max_length(X_test)\n",
    "max_lenght_oss_test = Max_length(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4af652ed-5759-4cce-8c21-1c54464057d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 25)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_rus, max_lenght_oss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "830d97c3-5032-4a1d-9cc8-4a5e113bfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_(text_data):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text_data)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer_input = tokenizer_(X_train)\n",
    "vocab_size_input = len(tokenizer_input.word_index) + 1\n",
    "tokenizer_target = tokenizer_(y_train)\n",
    "vocab_size_target = len(tokenizer_target.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9c2df04-8720-4c4f-b2d9-8900f4bd7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_input.pkl','wb') as f:\n",
    "    pkl.dump(tokenizer_input,f)\n",
    "\n",
    "with open('tokenizer_target.pkl','wb') as f:\n",
    "    pkl.dump(tokenizer_target,f)\n",
    "    \n",
    "pkl.dump(tokenizer_input, open('tokenizer_input.pkl', 'wb'))\n",
    "pkl.dump(tokenizer_target, open('tokenizer_target.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "403c63c9-7a66-403b-a79f-9e06fcee84e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(846, 856)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_input, vocab_size_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81ad8a6a-e1dc-48de-b446-c2d7ff443ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_batch(X= X_train,Y=y_train, batch_size=128):\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_data_input = np.zeros((batch_size,max_length_english),dtype='float32') #metrix of batch_size*max_length_english\n",
    "            decoder_data_input = np.zeros((batch_size,max_lenght_marathi),dtype='float32') #metrix of batch_size*max_length_marathi\n",
    "            decoder_target_input = np.zeros((batch_size,max_lenght_marathi,vocab_size_target),dtype='float32') # 3d array one hot encoder decoder target data\n",
    "            for i, (input_text,target_text) in enumerate(zip(X[j:j+batch_size],Y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_data_input[i,t] = tokenizer_input.word_index[word] # Here we are storing the encoder \n",
    "                                                                         #seq in row here padding is done automaticaly as \n",
    "                                                                         #we have defined col as max_lenght\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    # if word == 'START_':\n",
    "                    #   word = 'start'\n",
    "                    # elif word == 'END_':\n",
    "                    #   word = 'end'\n",
    "                    decoder_data_input[i,t] = tokenizer_target.word_index[word] # same for the decoder sequence\n",
    "                    if t>0:\n",
    "                        decoder_target_input[i,t-1,tokenizer_target.word_index[word]] = 1 #target is one timestep ahead of decoder input because it does not have 'start tag'\n",
    "            # print(encoder_data_input.shape())\n",
    "        yield ([encoder_data_input,decoder_data_input],decoder_target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cecaf5a-26f8-4b12-a372-d88732504b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,),name=\"encoder_inputs\")\n",
    "emb_layer_encoder = Embedding(vocab_size_input,latent_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(emb_layer_encoder)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,),name=\"decoder_inputs\")\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "emb_layer_decoder = Embedding(vocab_size_target,latent_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(emb_layer_decoder, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_target, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63153d0e-941a-4bf0-87eb-ae29652c09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ebbda12-686d-47de-af07-63661bd10b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, to_file='train_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d12c688a-25f0-426a-b9dd-873ee155dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec1d51b8-bb57-4260-a64f-1e08364e1a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ONIGAT~1\\AppData\\Local\\Temp/ipykernel_9480/1318473571.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator = generator_batch(X_train, y_train, batch_size = batch_size),\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<sos>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ONIGAT~1\\AppData\\Local\\Temp/ipykernel_9480/1318473571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model.fit_generator(generator = generator_batch(X_train, y_train, batch_size = batch_size),\n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_samples\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     epochs=epochs)\n",
      "\u001b[1;32mD:\\Programms\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2207\u001b[0m         \u001b[1;34m'Please use `Model.fit`, which supports generators.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2208\u001b[0m         stacklevel=2)\n\u001b[1;32m-> 2209\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   2210\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2211\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ONIGAT~1\\AppData\\Local\\Temp/ipykernel_9480/4237577856.py\u001b[0m in \u001b[0;36mgenerator_batch\u001b[1;34m(X, Y, batch_size)\u001b[0m\n\u001b[0;32m     15\u001b[0m                     \u001b[1;31m# elif word == 'END_':\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                     \u001b[1;31m#   word = 'end'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                     \u001b[0mdecoder_data_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# same for the decoder sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                         \u001b[0mdecoder_target_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m#target is one timestep ahead of decoder input because it does not have 'start tag'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '<sos>'"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator = generator_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db8682d-539a-4ace-9c95-3f414ee4fb04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
