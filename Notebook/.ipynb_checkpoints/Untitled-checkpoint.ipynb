{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39a8b505-6a3d-446a-843b-aee4531de74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e717dad7-bda9-4675-9e8c-0590bfac1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        \n",
    "        # Кол-во обычых токенов\n",
    "        self.n_words = 3\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    # Удалите слов ниже определенного порога подсчета\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed: return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words %s / %s = %.4f' % (\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b0a2fb3-412b-4f75-a7d2-d698234d9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([,.!?])\", r\"\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfac2d1c-6947-4125-8a26-0b849eef0fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Чтение строк...\")\n",
    "\n",
    "    filename = '%s-%s.txt' % (lang1, lang2)\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    ''' Разделение строк на пары и нормализация '''\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    ''' Переворот пар, создавание экземпляров класса Lang ''' \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70317eb0-4abd-4502-93e1-c747dd20b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "def filterPair(pairs):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[1].startswith(eng_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a6182db-e251-4a64-86ff-26b8e17c3f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чтение строк...\n",
      "Read 621481 sentence pairs\n",
      "Отфильтровано пар: 621481\n",
      "Индексирование слов...\n",
      "Индексированных 95135 слов на входном языке, 35714 слов на выходном\n",
      "['постучим по дереву чтобы наша мечта стала явью', \"let's knock on wood so that our dream will come true\"]\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse = False, filter_pairs_flag = False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print('Read %d sentence pairs' % len(pairs))\n",
    "    \n",
    "    if(filter_pairs_flag):\n",
    "        pairs = filterPair(pairs)\n",
    "    print('Отфильтровано пар: %d' % len(pairs))\n",
    "    \n",
    "    print('Индексирование слов...')\n",
    "    for pair in pairs:\n",
    "        for u in pair[0].split():\n",
    "            input_lang.addWord(u)\n",
    "        for u in pair[1].split():\n",
    "            output_lang.addWord(u)\n",
    "    \n",
    "    print('Индексированных %d слов на входном языке, %d слов на выходном' % (input_lang.n_words, output_lang.n_words))\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('rus', 'eng', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6c6304-96bf-4172-865e-4cb09ee9e3fa",
   "metadata": {},
   "source": [
    "## Фильтрация словарей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9045843d-c28c-4212-b8f6-44c5fcd44249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 95132 / 95132 = 1.0000\n",
      "keep_words 35711 / 35711 = 1.0000\n"
     ]
    }
   ],
   "source": [
    "''' Минимальное кол-во встречающихся слов, которых стоит оставить '''\n",
    "MIN_COUNT = 0\n",
    "\n",
    "input_lang.trim(MIN_COUNT)\n",
    "output_lang.trim(MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "842fd93d-0286-4f9c-8c6c-408f9eae3ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отброшено 621481 пар до 621119, 0.9994 от общего числа\n"
     ]
    }
   ],
   "source": [
    "keep_pairs = []\n",
    "\n",
    "for pair in pairs:\n",
    "    input_sentence = pair[0]\n",
    "    output_sentence = pair[1]\n",
    "    keep_input = True\n",
    "    keep_output = True\n",
    "    \n",
    "    for word in input_sentence.split(' '):\n",
    "        if word not in input_lang.word2index:\n",
    "            keep_input = False\n",
    "            break\n",
    "\n",
    "    for word in output_sentence.split(' '):\n",
    "        if word not in output_lang.word2index:\n",
    "            keep_output = False\n",
    "            break\n",
    "\n",
    "    ''' Удаление пар если они не соответствуют условиям ввода и вывода ''' \n",
    "    if keep_input and keep_output:\n",
    "        keep_pairs.append(pair)\n",
    "        \n",
    "print(\"Отброшено %d пар до %d, %.4f от общего числа\" % (len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "pairs = keep_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43522d35-f6e8-4745-8df4-46fd10c27b31",
   "metadata": {},
   "source": [
    "## Преобразование обучающих данных в тензоры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec168252-cb32-4860-84a4-ba884a48b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возвращает список индексов, по одному для каждого слова в предложении, плюс EOS '''\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')] + [EOS_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c83e5611-2314-4e57-92b2-d16f2e405cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполнение a символом PAD \n",
    "def padSEQ(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5acf6403-8b7f-449e-98e4-0683ae105f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomBatch(batch_size):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "\n",
    "    '''  Выбирайте случайные пары ''' \n",
    "    for i in range(batch_size):\n",
    "        pair = random.choice(pairs)\n",
    "        input_seqs.append(indexesFromSentence(input_lang, pair[0]))\n",
    "        target_seqs.append(indexesFromSentence(output_lang, pair[1]))\n",
    "\n",
    "    ''' Разделение на пары, сортировка по длине (по убыванию), разархивирование '''\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "    ''' Для входных и целевых последовательностей получим массив длин и заполните его от 0 до максимальной длины '''\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [padSEQ(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [padSEQ(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "    ''' Превратим дополненные массивы в тензоры (batch_size x max_len), транспонировать в (max_len x batch_size) '''\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "    target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "    \n",
    "    input_var = input_var.to(device)\n",
    "    target_var = target_var.to(device)\n",
    "        \n",
    "    return input_var, input_lengths, target_var, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19c914d3-409b-472b-962d-f6001a6349df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    7,   150],\n",
       "         [ 4121,   550],\n",
       "         [  915,  1808],\n",
       "         [    5,   374],\n",
       "         [ 8652,  9536],\n",
       "         [   73,  3575],\n",
       "         [   41,     2],\n",
       "         [    5,     0],\n",
       "         [24469,     0],\n",
       "         [    2,     0]]),\n",
       " [10, 7],\n",
       " tensor([[   20,    64],\n",
       "         [  107,   797],\n",
       "         [  434,   491],\n",
       "         [ 5079,    22],\n",
       "         [   92,    23],\n",
       "         [  150,  1507],\n",
       "         [  434,   228],\n",
       "         [11448,    23],\n",
       "         [    2,     2]]),\n",
       " [9, 9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomBatch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f3da6-d83a-4fce-a124-515bb9af3b05",
   "metadata": {},
   "source": [
    "# Построение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e15c80-452a-43c4-8a6a-d00049c9c98e",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9667a68-25ee-40b1-97cb-92bfcd0b8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        # Примечание: мы запускаем все это одновременно (в нескольких пакетах из нескольких последовательностей)\n",
    "        embedded = self.embedding(input_seqs).view(1, 1, -1)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        return outputs, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3397e132-7e18-4e2a-9206-692808f2e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Создаем переменную для хранения энергии внимания\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "\n",
    "        if USE_CUDA:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "        \n",
    "        # Для каждой партии выходов encoder'а\n",
    "        for b in range(this_batch_size):\n",
    "            # Вычисляет энергию для каждого выходного сигнала encoder'а\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Нормализуйте энергии до весов в диапазоне от 0 до 1, измените размер до 1 x B x S\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        if self.method == 'dot':\n",
    "            energy =torch.dot(hidden.view(-1), encoder_output.view(-1))\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = torch.dot(hidden.view(-1), energy.view(-1))\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = torch.dot(self.v.view(-1), energy.view(-1))\n",
    "        return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e1a4650-972e-4e9f-884b-dcd8e5f16573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Сохранение для справки\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Определение слоев\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Выбор модели внимания\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "\n",
    "        # Получаем вложение текущего входного слова (последнее выходное слово)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "\n",
    "        # Получаем текущее скрытое состояние из входного слова и последнее скрытое состояние\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Вычисление внимание по текущему состоянию RNN и всем выходам  encoder'а\n",
    "        # Примение к выходам encoder'а для получения средневзвешенного значения\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
    "\n",
    "        # Вектор внимания с использованием скрытого состояния RNN и вектора контекста\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Возвращение конечного результата, скрытого состояния и веса внимания (для визуализации)\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1055e8b1-4d53-4d44-9e52-785d66fba95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "709c5f1f-8625-4b5e-8a6c-ab62a33cb96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batches torch.Size([9, 2])\n",
      "target_batches torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "small_batch_size = 2\n",
    "input_batches, input_lengths, target_batches, target_lengths = randomBatch(small_batch_size)\n",
    "\n",
    "print('input_batches', input_batches.size()) # (max_len x batch_size)\n",
    "print('target_batches', target_batches.size()) # (max_len x batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1d6cde7-2490-4ac2-8495-c8a283dfd4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(95135, 8)\n",
      "  (gru): GRU(8, 8, num_layers=2, dropout=0.1, bidirectional=True)\n",
      ")\n",
      "DecoderRNN(\n",
      "  (embedding): Embedding(35714, 8)\n",
      "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gru): GRU(8, 8, num_layers=2, dropout=0.1)\n",
      "  (concat): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (out): Linear(in_features=8, out_features=35714, bias=True)\n",
      "  (attn): Attn(\n",
      "    (attn): Linear(in_features=8, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "small_hidden_size = 8\n",
    "small_n_layers = 2\n",
    "\n",
    "encoder_test = EncoderRNN(input_lang.n_words, small_hidden_size, small_n_layers)\n",
    "decoder_test = DecoderRNN('general', small_hidden_size, output_lang.n_words, small_n_layers)\n",
    "\n",
    "print(encoder_test.to(device))\n",
    "print(decoder_test.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b37a0-98ce-4a76-bbb6-bc192c8ab18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ddb080-9d97-4e94-9b02-2338b1ba39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac376f-0e9b-4cc6-bd5e-349a6bc3e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e2f1f8-cdb2-4af4-8a7f-2b244cfa9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc25e94-fe01-499b-97c5-cb2ce3112990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ad6fe-26c9-4a1d-acdd-34c1b12a834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 15000, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c4329-629e-4b48-bc67-239e4a7afefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351324a-a97d-4059-9f68-977783f8fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateA(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae5a59e-c582-4ef8-8a26-733746fef012",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateA('mather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d19699-1484-46fe-98b0-61f1d5be140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(EncoderRNN.state_dict(), 'C:\\\\Users\\\\Onigatari\\\\Desktop\\\\Graduate-Work\\\\Notebook')\n",
    "torch.save(DecoderRNN.state_dict(), 'C:\\\\Users\\\\Onigatari\\\\Desktop\\\\Graduate-Work\\\\Notebook')\n",
    "torch.save(AttnDecoderRNN.state_dict(), 'C:\\\\Users\\\\Onigatari\\\\Desktop\\\\Graduate-Work\\\\Notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e7b7e-5264-4691-9b2c-f8688c5e3468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
