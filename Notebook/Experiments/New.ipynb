{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76c280c9-fab4-4b94-b722-aa651971fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "028c4405-a328-4362-9755-54ca4ebcec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_builtins = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbd90ac8-3686-4ee7-aa82-b314f079d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        # Keep a cache of every axis-name seen\n",
    "        self.shapes = {}\n",
    "\n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "\n",
    "        if isinstance(names, str):\n",
    "            names = (names,)\n",
    "\n",
    "        shape = tf.shape(tensor)\n",
    "        rank = tf.rank(tensor)\n",
    "\n",
    "        if rank != len(names):\n",
    "            raise ValueError(f'Rank mismatch:\\n'\n",
    "                       f'    found {rank}: {shape.numpy()}\\n'\n",
    "                       f'    expected {len(names)}: {names}\\n')\n",
    "\n",
    "        for i, name in enumerate(names):\n",
    "            if isinstance(name, int):\n",
    "                old_dim = name\n",
    "            else:\n",
    "                old_dim = self.shapes.get(name, None)\n",
    "            new_dim = shape[i]\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "\n",
    "        if new_dim != old_dim:\n",
    "            raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                         f\"    found: {new_dim}\\n\"\n",
    "                         f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b6713fd-cdc8-49a8-a944-cdab577b861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, reverse=True):\n",
    "    with open(path, \"r\", encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "    inp = [inp for targ, inp in pairs]\n",
    "    targ = [targ for targ, inp, in pairs]\n",
    "    \n",
    "    if reverse:\n",
    "        inp, targ = targ, inp\n",
    "    return inp, targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb96d729-0dc5-4d8a-943c-76eaec038ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Go.', 'Ve.')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file = 'Date/eng-spa.txt'\n",
    "inp, targ = load_data(path_to_file)\n",
    "\n",
    "inp[0], targ[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd640728-5014-49c3-ad92-bb2ac8116f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d9a0c47-ec72-4510-9437-f696a5d03cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"I don't like that word.\" b'Tom is still young.'\n",
      " b'He did it not once, but twice.' b'Please excuse my being late.'\n",
      " b'This is very good.'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'No me gusta esa palabra.' b'Tom\\xc3\\xa1s todav\\xc3\\xada es joven.'\n",
      " b'No lo hizo una sola vez, sino dos.' b'Perd\\xc3\\xb3n por el retraso.'\n",
      " b'Esto es muy bueno.'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "    print(example_input_batch[:5])\n",
    "    print()\n",
    "    print(example_target_batch[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c61da90-c6ad-496d-9246-a97f6b711279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accecented characters.\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e2efc5a-77bb-4cd1-899f-70e44d7a94fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет, мой друг!\n",
      "[START] ,    ! [END]\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('Привет, мой друг!')\n",
    "\n",
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6cf93a0-fbee-4a79-821b-655ed7d72bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1bea7be-e639-4b8f-9666-807125a212a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'the', 'i', 'to', 'you', 'tom']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.adapt(inp)\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "input_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6df49c37-3207-4766-b21a-106b56cb45e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)\n",
    "\n",
    "output_text_processor.adapt(targ)\n",
    "output_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fd74f39-58a3-4dd7-b0da-64d4ab1a17cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
       "array([[  2,   6,  27,  41,  16, 453,   4,   3,   0,   0],\n",
       "       [  2,   9,  12, 161, 327,   4,   3,   0,   0,   0],\n",
       "       [  2,  13,  55,  17,  38, 341,  19,  94, 781,   4]], dtype=int64)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokens = input_text_processor(example_input_batch)\n",
    "example_tokens[:3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a848a0db-710b-43d0-bed1-8d2e9a99a909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] i dont like that word . [END]            '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c37731d-6fd8-4acf-b6f6-fe0d0cde1a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(example_tokens)\n",
    "plt.title('Token IDs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens != 0)\n",
    "plt.title('Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce7a51-b2d0-4e9c-98bc-0a0c4d13a903",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e424edb3-7b7e-4782-b7d8-565addabc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f7a3332-0f5a-4557-9b0a-763ce13b448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                                   embedding_dim)\n",
    "\n",
    "        # The GRU RNN layer processes those vectors sequentially.\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       # Return the sequence and state\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding for each token.\n",
    "        vectors = self.embedding(tokens)\n",
    "        shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "        # 3. The GRU processes the embedding sequence.\n",
    "        #    output shape: (batch, s, enc_units)\n",
    "        #    state shape: (batch, enc_units)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "        shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "        # 4. Returns the new sequence and its state.\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17edf399-28d4-477c-b654-c9db6348023b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"encoder_2\" (type Encoder).\n\nShape mismatch for dimension: 's'\n    found: 20\n    expected: None\n\n\nCall arguments received:\n  • tokens=tf.Tensor(shape=(64, 20), dtype=int64)\n  • state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ONIGAT~1\\AppData\\Local\\Temp/ipykernel_14440/2845961637.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m encoder = Encoder(input_text_processor.vocabulary_size(),\n\u001b[0;32m      6\u001b[0m                   embedding_dim, units)\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mexample_enc_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample_enc_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Input batch, shape (batch): {example_input_batch.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ONIGAT~1\\AppData\\Local\\Temp/ipykernel_14440/720237546.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, tokens, state)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mshape_checker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mShapeChecker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mshape_checker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m's'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# 2. The embedding layer looks up the embedding for each token.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ONIGAT~1\\AppData\\Local\\Temp/ipykernel_14440/483058825.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, tensor, names, broadcast)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_dim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mold_dim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n\u001b[0m\u001b[0;32m     38\u001b[0m                          \u001b[1;34mf\"    found: {new_dim}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                          f\"    expected: {old_dim}\\n\")\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"encoder_2\" (type Encoder).\n\nShape mismatch for dimension: 's'\n    found: 20\n    expected: None\n\n\nCall arguments received:\n  • tokens=tf.Tensor(shape=(64, 20), dtype=int64)\n  • state=None"
     ]
    }
   ],
   "source": [
    "# Convert the input text to tokens.\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "\n",
    "# Encode the input sequence.\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
    "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e9ee85-5dc3-4faf-9bd0-de136f140aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # For Eqn. (4), the  Bahdanau attention\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(query, ('batch', 't', 'query_units'))\n",
    "        shape_checker(value, ('batch', 's', 'value_units'))\n",
    "        shape_checker(mask, ('batch', 's'))\n",
    "\n",
    "        # From Eqn. (4), `W1@ht`.\n",
    "        w1_query = self.W1(query)\n",
    "        shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
    "\n",
    "        # From Eqn. (4), `W2@hs`.\n",
    "        w2_key = self.W2(value)\n",
    "        shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs = [w1_query, value, w2_key],\n",
    "            mask=[query_mask, value_mask],\n",
    "            return_attention_scores = True,\n",
    "        )\n",
    "        shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ecd191-03d7-4e29-b875-9b2e15d25c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552d8e4-ea07-4c8b-bd52-387ae8409a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "(example_tokens != 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3521c-5681-4c10-92cd-d40ebb04e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later, the decoder will generate this attention query\n",
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "\n",
    "context_vector, attention_weights = attention_layer(\n",
    "    query=example_attention_query,\n",
    "    value=example_enc_output,\n",
    "    mask=(example_tokens != 0))\n",
    "\n",
    "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
    "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4fe726-68f7-4634-b4f3-be3aed102307",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(attention_weights[:, 0, :])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens != 0)\n",
    "plt.title('Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7815c-08ef-4851-8031-f37d6eb58387",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343567d-2817-4732-89f5-9bca1a4199b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_slice = attention_weights[0, 0].numpy()\n",
    "attention_slice = attention_slice[attention_slice != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c524c27-dbc4-47f6-a103-5825f3cf83ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "plt.suptitle('Attention weights for one sequence')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "a1 = plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(attention_slice)), attention_slice)\n",
    "# freeze the xlim\n",
    "plt.xlim(plt.xlim())\n",
    "plt.xlabel('Attention weights')\n",
    "\n",
    "a2 = plt.subplot(1, 2, 2)\n",
    "plt.bar(range(len(attention_slice)), attention_slice)\n",
    "plt.xlabel('Attention weights, zoomed')\n",
    "\n",
    "# zoom in\n",
    "top = max(a1.get_ylim())\n",
    "zoom = 0.85*top\n",
    "a2.set_ylim([0.90*top, top])\n",
    "a1.plot(a1.get_xlim(), [zoom, zoom], color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4707ade-1c0c-452f-a3b7-387bde2b83cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # For Step 1. The embedding layer convets token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
    "                                                   embedding_dim)\n",
    "\n",
    "        # For Step 2. The RNN keeps track of what's been generated so far.\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # For step 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "        # For step 4. Eqn. (3): converting `ct` to `at`\n",
    "        self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
    "                                        use_bias=False)\n",
    "\n",
    "        # For step 5. This fully connected layer produces the logits for each\n",
    "        # output token.\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d377b6-9c21-4af2-80d8-b0ee5995b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844389fb-f91d-4c7d-9c99-2598b1f7ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self,\n",
    "         inputs: DecoderInput,\n",
    "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(inputs.new_tokens, ('batch', 't'))\n",
    "    shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
    "    shape_checker(inputs.mask, ('batch', 's'))\n",
    "\n",
    "    if state is not None:\n",
    "        shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "    # Step 1. Lookup the embeddings\n",
    "    vectors = self.embedding(inputs.new_tokens)\n",
    "    shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
    "\n",
    "    # Step 2. Process one step with the RNN\n",
    "    rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "    shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
    "    shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "    # Step 3. Use the RNN output as the query for the attention over the\n",
    "    # encoder output.\n",
    "    context_vector, attention_weights = self.attention(\n",
    "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
    "    shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
    "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "    # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
    "    #     [ct; ht] shape: (batch t, value_units + query_units)\n",
    "    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "\n",
    "    # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
    "    attention_vector = self.Wc(context_and_rnn_output)\n",
    "    shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
    "\n",
    "    # Step 5. Generate logit predictions:\n",
    "    logits = self.fc(attention_vector)\n",
    "    shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
    "\n",
    "    return DecoderOutput(logits, attention_weights), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c774ab96-9c3f-4b9e-b449-98e4d6411f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder.call = call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8b062-bc72-47de-9cc0-fdaab338d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcce67b-f51a-4089-8465-71728b3119c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target sequence, and collect the \"[START]\" tokens\n",
    "example_output_tokens = output_text_processor(example_target_batch)\n",
    "\n",
    "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500fac0a-40e7-4469-ba92-efbc64942e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the decoder\n",
    "dec_result, dec_state = decoder(\n",
    "    inputs = DecoderInput(new_tokens=first_token,\n",
    "                          enc_output=example_enc_output,\n",
    "                          mask=(example_tokens != 0)),\n",
    "    state = example_enc_state\n",
    ")\n",
    "\n",
    "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
    "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913ff8e3-3bc0-4dd5-8c8e-495a4d970da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc960d1-bf2e-4c77-b061-882879ddfa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(output_text_processor.get_vocabulary())\n",
    "first_word = vocab[sampled_token.numpy()]\n",
    "first_word[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8befcc-945f-4866-ba03-e4228ff222a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_result, dec_state = decoder(\n",
    "    DecoderInput(sampled_token,\n",
    "                 example_enc_output,\n",
    "                 mask=(example_tokens != 0)),\n",
    "    state=dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ba810-6898-4f88-ab6d-7bebc2e12b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
    "first_word = vocab[sampled_token.numpy()]\n",
    "first_word[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c67186-1e64-4349-8abe-4affcec48b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = 'masked_loss'\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(y_true, ('batch', 't'))\n",
    "        shape_checker(y_pred, ('batch', 't', 'logits'))\n",
    "\n",
    "        # Calculate the loss for each item in the batch.\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        shape_checker(loss, ('batch', 't'))\n",
    "\n",
    "        # Mask off the losses on padding.\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\n",
    "        shape_checker(mask, ('batch', 't'))\n",
    "        loss *= mask\n",
    "\n",
    "        # Return the total.\n",
    "        return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f385018-559f-4d63-8035-ff0ccb54fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units,\n",
    "               input_text_processor,\n",
    "               output_text_processor, \n",
    "               use_tf_function=True):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                          embedding_dim, units)\n",
    "        decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                          embedding_dim, units)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        self.use_tf_function = use_tf_function\n",
    "    self.shape_checker = ShapeChecker()\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        self.shape_checker = ShapeChecker()\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f60a9f6-fbf2-498a-b5d5-750a25c62ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(self, input_text, target_text):\n",
    "    self.shape_checker(input_text, ('batch',))\n",
    "    self.shape_checker(target_text, ('batch',))\n",
    "\n",
    "    # Convert the text to token IDs\n",
    "    input_tokens = self.input_text_processor(input_text)\n",
    "    target_tokens = self.output_text_processor(target_text)\n",
    "    self.shape_checker(input_tokens, ('batch', 's'))\n",
    "    self.shape_checker(target_tokens, ('batch', 't'))\n",
    "\n",
    "    # Convert IDs to masks.\n",
    "    input_mask = input_tokens != 0\n",
    "    self.shape_checker(input_mask, ('batch', 's'))\n",
    "\n",
    "    target_mask = target_tokens != 0\n",
    "    self.shape_checker(target_mask, ('batch', 't'))\n",
    "\n",
    "    return input_tokens, input_mask, target_tokens, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e631a12-bd87-497e-aa9e-e82477722000",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._preprocess = _preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0623843f-771f-4ff7-983f-48cfabeba5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_step(self, inputs):\n",
    "    input_text, target_text = inputs  \n",
    "\n",
    "    (input_tokens, input_mask,\n",
    "    target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "\n",
    "    max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Encode the input\n",
    "        enc_output, enc_state = self.encoder(input_tokens)\n",
    "        self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
    "        self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
    "\n",
    "        # Initialize the decoder's state to the encoder's final state.\n",
    "        # This only works if the encoder and decoder have the same number of\n",
    "        # units.\n",
    "        dec_state = enc_state\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "        for t in tf.range(max_target_length-1):\n",
    "            # Pass in two tokens from the target sequence:\n",
    "            # 1. The current input to the decoder.\n",
    "            # 2. The target for the decoder's next prediction.\n",
    "            new_tokens = target_tokens[:, t:t+2]\n",
    "            step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                                 enc_output, dec_state)\n",
    "            loss = loss + step_loss\n",
    "\n",
    "        # Average the loss over all non padding tokens.\n",
    "        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "    # Apply an optimization step\n",
    "    variables = self.trainable_variables \n",
    "    gradients = tape.gradient(average_loss, variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # Return a dict mapping metric names to current value\n",
    "    return {'batch_loss': average_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199baee-f534-43f9-b551-45a9b4258ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._train_step = _train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf95b52-939a-4ae2-8033-4faab879381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "\n",
    "    # Run the decoder one step.\n",
    "    decoder_input = DecoderInput(new_tokens=input_token,\n",
    "                               enc_output=enc_output,\n",
    "                               mask=input_mask)\n",
    "\n",
    "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "    self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
    "    self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
    "    self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
    "\n",
    "    # `self.loss` returns the total for non-padded tokens\n",
    "    y = target_token\n",
    "    y_pred = dec_result.logits\n",
    "    step_loss = self.loss(y, y_pred)\n",
    "\n",
    "    return step_loss, dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a5cb6-6301-4749-94f6-35504be972ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._loop_step = _loop_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12525f-027c-4e95-8376-e5a238377ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(output_text_processor.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83168bd2-a6e1-45fe-a3c4-1c830a9703c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for n in range(10):\n",
    "    print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187f49c-059e-43c1-b8d6-8d92df25fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
    "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
    "def _tf_train_step(self, inputs):\n",
    "    return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480291fd-70ee-482e-a198-0224eb474a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._tf_train_step = _tf_train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef14b1-4cc2-40e2-99c3-37307031389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.use_tf_function = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec926d4-189d-4c98-89ad-c06fdcffc42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.train_step([example_input_batch, example_target_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4122a0-b748-4be2-baec-1e707f83ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for n in range(10):\n",
    "    print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f9f94e-6a53-4fdc-bc4a-f5b6eef7c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for n in range(100):\n",
    "    print('.', end='')\n",
    "    logs = translator.train_step([example_input_batch, example_target_batch])\n",
    "    losses.append(logs['batch_loss'].numpy())\n",
    "\n",
    "print()\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f370eac-6d5d-42d1-aa97-cb95c57c756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "train_translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dabfcf-e4c7-4817-9bd4-44af281e33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.logs = []\n",
    "\n",
    "    def on_train_batch_end(self, n, logs):\n",
    "        self.logs.append(logs[self.key])\n",
    "\n",
    "batch_loss = BatchLogs('batch_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8becb0-8bbf-48e1-bf02-238db89648a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_translator.fit(dataset, epochs=3,\n",
    "                     callbacks=[batch_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22ef9b-78cf-4394-9664-0ac05be557eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(batch_loss.logs)\n",
    "plt.ylim([0, 3])\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('CE/token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a5578e-ab84-45b2-b8b0-496b14fe3e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, input_text_processor,\n",
    "               output_text_processor):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "\n",
    "        self.output_token_string_from_index = (\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=output_text_processor.get_vocabulary(),\n",
    "                mask_token='',\n",
    "                invert=True))\n",
    "\n",
    "        # The output should never generate padding, unknown, or start.\n",
    "        index_from_string = tf.keras.layers.StringLookup(\n",
    "            vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
    "        token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
    "\n",
    "        token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
    "        token_mask[np.array(token_mask_ids)] = True\n",
    "        self.token_mask = token_mask\n",
    "\n",
    "        self.start_token = index_from_string(tf.constant('[START]'))\n",
    "        self.end_token = index_from_string(tf.constant('[END]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01a45d-6da1-4b1b-a638-5cc01b19340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(\n",
    "    encoder=train_translator.encoder,\n",
    "    decoder=train_translator.decoder,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460aa269-c965-44e4-b822-0bab220c9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(self, result_tokens):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(result_tokens, ('batch', 't'))\n",
    "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "    shape_checker(result_text_tokens, ('batch', 't'))\n",
    "\n",
    "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
    "                                       axis=1, separator=' ')\n",
    "    shape_checker(result_text, ('batch'))\n",
    "\n",
    "    result_text = tf.strings.strip(result_text)\n",
    "    shape_checker(result_text, ('batch',))\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fed0d-a704-4c24-b511-1570f1f5046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.tokens_to_text = tokens_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bcbaa7-1e95-4757-a7b9-12d4ff839b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output_tokens = tf.random.uniform(\n",
    "    shape=[5, 2], minval=0, dtype=tf.int64,\n",
    "    maxval=output_text_processor.vocabulary_size())\n",
    "translator.tokens_to_text(example_output_tokens).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8342826-3a34-4277-9f61-25849608aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(self, logits, temperature):\n",
    "    shape_checker = ShapeChecker()\n",
    "    # 't' is usually 1 here.\n",
    "    shape_checker(logits, ('batch', 't', 'vocab'))\n",
    "    shape_checker(self.token_mask, ('vocab',))\n",
    "\n",
    "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
    "    shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
    "\n",
    "    # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
    "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        new_tokens = tf.argmax(logits, axis=-1)\n",
    "    else: \n",
    "        logits = tf.squeeze(logits, axis=1)\n",
    "        new_tokens = tf.random.categorical(logits/temperature,\n",
    "                                        num_samples=1)\n",
    "\n",
    "    shape_checker(new_tokens, ('batch', 't'))\n",
    "\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb7798a-e147-4e33-b80a-28f39927b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.sample = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddebdc2-4f1d-431f-8aad-e63315ef3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
    "example_output_tokens = translator.sample(example_logits, temperature=1.0)\n",
    "example_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47aaba7-a159-4b2d-a9df-484b9fbb57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_unrolled(self,\n",
    "                       input_text, *,\n",
    "                       max_length=50,\n",
    "                       return_attention=True,\n",
    "                       temperature=1.0):\n",
    "    batch_size = tf.shape(input_text)[0]\n",
    "    input_tokens = self.input_text_processor(input_text)\n",
    "    enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "    dec_state = enc_state\n",
    "    new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "\n",
    "    result_tokens = []\n",
    "    attention = []\n",
    "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        dec_input = DecoderInput(new_tokens=new_tokens,\n",
    "                                     enc_output=enc_output,\n",
    "                                     mask=(input_tokens!=0))\n",
    "    \n",
    "        dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
    "\n",
    "        attention.append(dec_result.attention_weights)\n",
    "\n",
    "        new_tokens = self.sample(dec_result.logits, temperature)\n",
    "\n",
    "        # If a sequence produces an `end_token`, set it `done`\n",
    "        done = done | (new_tokens == self.end_token)\n",
    "        # Once a sequence is done it only produces 0-padding.\n",
    "        new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
    "\n",
    "        # Collect the generated tokens\n",
    "        result_tokens.append(new_tokens)\n",
    "\n",
    "        if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "            break\n",
    "\n",
    "    # Convert the list of generates token ids to a list of strings.\n",
    "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
    "    result_text = self.tokens_to_text(result_tokens)\n",
    "\n",
    "    if return_attention:\n",
    "        attention_stack = tf.concat(attention, axis=1)\n",
    "        return {'text': result_text, 'attention': attention_stack}\n",
    "    else:\n",
    "        return {'text': result_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc530e-cec8-47d8-bed1-a47a824b4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.translate = translate_unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce68ed-8e42-4215-ba56-1df04172cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_text = tf.constant([\n",
    "    'hace mucho frio aqui.', # \"It's really cold here.\"\n",
    "    'Esta es mi vida.', # \"This is my life.\"\"\n",
    "])\n",
    "\n",
    "result = translator.translate(\n",
    "    input_text = input_text)\n",
    "\n",
    "print(result['text'][0].numpy().decode())\n",
    "print(result['text'][1].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a88043f-8e02-4dd0-987c-3ed2660a3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "def tf_translate(self, input_text):\n",
    "    return self.translate(input_text)\n",
    "\n",
    "Translator.tf_translate = tf_translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a649dd84-86d0-4955-bba8-04ea76437bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = translator.tf_translate(\n",
    "    input_text = input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2750e5-0972-4a70-8a62-4c8a8c9c1aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = translator.tf_translate(\n",
    "    input_text = input_text)\n",
    "\n",
    "print(result['text'][0].numpy().decode())\n",
    "print(result['text'][1].numpy().decode())\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
