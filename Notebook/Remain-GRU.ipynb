{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3bd3b4-219b-4c3b-a7ee-61d253092206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re, os, time, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle as pkl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import LSTM, GRU, Input, Dense, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d99dca-d5f9-47c8-bb27-2cd56b9b6926",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1337\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2628a3b-41a1-4f5a-ae0b-f0b86b334101",
   "metadata": {},
   "source": [
    "# Подготовка даных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ddd6f5b-f224-40b9-bf48-917cb4828503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(data, punctuation=False, add_tokens=False):\n",
    "    \"\"\"\n",
    "        Функция для предобработки \n",
    "    \"\"\"\n",
    "    \n",
    "    # Уменьшаем регистр и убираем лишние пробелы\n",
    "    data = [w.lower().strip() for w in data]\n",
    "    \n",
    "    # Замена всех символов 'æ' на однотипный\n",
    "    data = [re.sub(r\"ӕ\", r\"æ\", w) for w in data]\n",
    "    \n",
    "    # Удаление апострофом\n",
    "    data = [re.sub(\"'\", '', w) for w in data]\n",
    "    \n",
    "    if punctuation:\n",
    "        # Делаем между словом и знаком пунктуации отступ 'слово! -> слово !'\n",
    "        data = [re.sub(r\"([?.!,])\", r\" \\1 \", w) for w in data]\n",
    "        data = [re.sub(r'[\" \"]+', \" \", w) for w in data]\n",
    "    else:\n",
    "        # Удаляет все знаки пунктуации\n",
    "        data = [re.sub(r\"[^\\w\\s]\", r\"\", w) for w in data]\n",
    "    \n",
    "    # Выкидываем все остальные символы из рассмотрения \n",
    "    data = [re.sub(r\"[^a-яА-Яa-zA-Z?.!,æё]+\", \" \", w) for w in data]\n",
    "    data = [w.rstrip().strip() for w in data]\n",
    "    \n",
    "    # Добавляем токены для начала и конца предложения\n",
    "    if add_tokens:\n",
    "        data = [f'{start_target} {w} {end_target}' for w in data]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06889a21-0434-4753-883e-bf24d25b3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path, preparing=False):\n",
    "    \"\"\"\n",
    "        Функция для создания датасета\n",
    "    \"\"\"\n",
    "    \n",
    "    # Открытие файла с данными в кодировке UTF-8\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    # Разделение данных на пары <<предложение, перевод>>\n",
    "    uncleaned_data_list = data.split('\\n')\n",
    "    \n",
    "    source_word = []\n",
    "    target_word = []\n",
    "    for word in uncleaned_data_list:\n",
    "        source_word.append(word.split('\\t')[0])\n",
    "        target_word.append(word.split('\\t')[1])\n",
    "    \n",
    "    # Инецализация датафрейма pandas\n",
    "    language_data = pd.DataFrame(columns=['Source','Target'])\n",
    "    if preparing:\n",
    "        language_data['Source'] = preprocess_sentence(source_word)\n",
    "        language_data['Target'] = preprocess_sentence(target_word, add_tokens=True)\n",
    "    else:\n",
    "        language_data['Source'] = source_word\n",
    "        language_data['Target'] = target_word\n",
    "        \n",
    "    return language_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a3d01f-6287-43ee-8672-526b327e7dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text_data):\n",
    "    '''\n",
    "        Токенайзер\n",
    "    '''\n",
    "    \n",
    "    tokenizer = Tokenizer(filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(text_data)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "298ea3a1-6e0b-4994-b6be-3e235160b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data(path):\n",
    "    data = read_dataset(path, preparing=True)\n",
    "    tokenizer_input, tokenizer_output = tokenize(data['Source'].values), tokenize(data['Target'].values)\n",
    "    input_max_length, output_max_length = len(tokenizer_input.word_index) + 1, len(tokenizer_output.word_index) + 1\n",
    "    \n",
    "    return data, tokenizer_input, tokenizer_output, input_max_length, output_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138a1e2-7510-4933-b3f1-59d31d2efd0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Построение модели Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b68f6f-6c03-4d2a-8828-b99c366888d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "        Энкодер \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size_input, HIDDEN_DIM):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.inputs = Input(shape=(None,), name=\"encoder_inputs\")\n",
    "        self.embedding = Embedding(vocab_size_input, HIDDEN_DIM, mask_zero=True, name=\"encoder_embedding\")(self.inputs)\n",
    "        \n",
    "        self.encoder = GRU(HIDDEN_DIM, return_state=True, name=\"encoder_gru\")\n",
    "        self.outputs, state_h = self.encoder(self.embedding)\n",
    "        self.states = [state_h]\n",
    "    \n",
    "    @staticmethod\n",
    "    def getModel(model):\n",
    "        inputs_inf = model.input[0]\n",
    "        outputs_inf, inf_state_h = model.layers[4].output\n",
    "        inf_states = [inf_state_h]\n",
    "        \n",
    "        return Model(inputs_inf, inf_states, name='Encoder') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "454a1f31-5c8f-4274-9b7e-b6a712e855a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "        Декодер \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size_output, HIDDEN_DIM, encoder_states):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.inputs = Input(shape=(None,), name=\"decoder_inputs\")\n",
    "        self.embedding = Embedding(vocab_size_output, HIDDEN_DIM, mask_zero=True, name=\"decoder_embedding\")(self.inputs)\n",
    "        \n",
    "        self.decoder = GRU(HIDDEN_DIM, return_sequences=True, return_state=True, name=\"decoder_gru\")\n",
    "        self.outputs, _ = self.decoder(self.embedding, initial_state=encoder_states)\n",
    "        self.dense = Dense(vocab_size_output, activation='softmax', name=\"dense_gru\")\n",
    "        self.outputs = self.dense(self.outputs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def getModel(model):\n",
    "        state_h_input = Input(shape=(HIDDEN_DIM,))\n",
    "        state_input = [state_h_input]\n",
    "\n",
    "        input_inf = model.input[1]\n",
    "        emb_inf = model.layers[3](input_inf)\n",
    "        lstm_inf = model.layers[5]\n",
    "        output_inf, state_h_inf = lstm_inf(emb_inf, initial_state=state_input)\n",
    "        state_inf = [state_h_inf]\n",
    "        dense_inf = model.layers[6]\n",
    "        output_final = dense_inf(output_inf)\n",
    "\n",
    "        return Model([input_inf]+state_input, [output_final]+state_inf, name='Decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81ad8a6a-e1dc-48de-b446-c2d7ff443ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatorBatch(X, Y, batch_size):\n",
    "    max_length = lambda data: max([len(x.split(' ')) for x in data])\n",
    "    \n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_data_input   = np.zeros((batch_size, max_length(X)), dtype='float32')\n",
    "            decoder_data_input   = np.zeros((batch_size, max_length(Y)), dtype='float32')\n",
    "            decoder_target_input = np.zeros((batch_size, max_length(Y), vocab_size_target), dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j : j + batch_size], Y[ j : j + batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_data_input[i, t] = tokenizer_input.word_index[word]\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    decoder_data_input[i, t] = tokenizer_output.word_index[word]\n",
    "                    if t>0:\n",
    "                        decoder_target_input[i,t - 1,tokenizer_output.word_index[word]] = 1\n",
    "            yield ([encoder_data_input, decoder_data_input], decoder_target_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a70c8d-84ee-44a6-ba55-6a17657e615d",
   "metadata": {},
   "source": [
    "# Входные параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f10c3abb-6cec-444f-b8fc-dd3b5c769a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Date/rus-oss.txt'\n",
    "\n",
    "start_target = \"<sos>\"\n",
    "end_target = \"<eos>\"\n",
    "\n",
    "HIDDEN_DIM = 256\n",
    "batch_size = 6\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e15cd9ae-e669-4474-9ea3-86744e9fb55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, tokenizer_input, tokenizer_output, vocab_size_source, vocab_size_target = preparing_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bc0848b-e173-4914-84c0-aef68b3845cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['Source'].values, data['Target'].values, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c75ca18-873c-427b-94c2-23383c2a91d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Чего ты смеёшься?</td>\n",
       "      <td>Цæуыл худыс?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Этот нож очень острый.</td>\n",
       "      <td>Ацы кард тынг цыргъ у.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>У кошки девять жизней.</td>\n",
       "      <td>Гæдыйæн фараст царды ис.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Сегодня облачно.</td>\n",
       "      <td>Абон у асæст.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Он был вождём своего племени 35 лет.</td>\n",
       "      <td>Уый йæ знæмы раздзог уыдис 35 азы дæргъы.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Source  \\\n",
       "0                     Чего ты смеёшься?   \n",
       "1                Этот нож очень острый.   \n",
       "2                У кошки девять жизней.   \n",
       "3                      Сегодня облачно.   \n",
       "4  Он был вождём своего племени 35 лет.   \n",
       "\n",
       "                                      Target  \n",
       "0                               Цæуыл худыс?  \n",
       "1                     Ацы кард тынг цыргъ у.  \n",
       "2                   Гæдыйæн фараст царды ис.  \n",
       "3                              Абон у асæст.  \n",
       "4  Уый йæ знæмы раздзог уыдис 35 азы дæргъы.  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_dataset(path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c392401-db26-48b5-9ec1-9e48131652a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>чего ты смеёшься</td>\n",
       "      <td>&lt;sos&gt; цæуыл худыс &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>этот нож очень острый</td>\n",
       "      <td>&lt;sos&gt; ацы кард тынг цыргъ у &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>у кошки девять жизней</td>\n",
       "      <td>&lt;sos&gt; гæдыйæн фараст царды ис &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>сегодня облачно</td>\n",
       "      <td>&lt;sos&gt; абон у асæст &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>он был вождём своего племени лет</td>\n",
       "      <td>&lt;sos&gt; уый йæ знæмы раздзог уыдис азы дæргъы &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Source  \\\n",
       "0                  чего ты смеёшься   \n",
       "1             этот нож очень острый   \n",
       "2             у кошки девять жизней   \n",
       "3                   сегодня облачно   \n",
       "4  он был вождём своего племени лет   \n",
       "\n",
       "                                              Target  \n",
       "0                            <sos> цæуыл худыс <eos>  \n",
       "1                  <sos> ацы кард тынг цыргъ у <eos>  \n",
       "2                <sos> гæдыйæн фараст царды ис <eos>  \n",
       "3                           <sos> абон у асæст <eos>  \n",
       "4  <sos> уый йæ знæмы раздзог уыдис азы дæргъы <eos>  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6747ab3-2dfc-4fe5-9bdc-7ff2b3eb1f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тренировочная выборка: 423\n",
      "Тестовая выборка: 47\n"
     ]
    }
   ],
   "source": [
    "print(f'Тренировочная выборка: {len(X_train)}')\n",
    "print(f'Тестовая выборка: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8fb90c3-e972-4fa3-94ca-0b3c1067b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size_source, HIDDEN_DIM)\n",
    "decoder = Decoder(vocab_size_target, HIDDEN_DIM, encoder.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3707bd35-a98a-415b-97b9-4b3e2057f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder.inputs, decoder.inputs], decoder.outputs, name=\"Seq2Seq-GRU-Translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c28ea3a-7a69-4e5b-a24a-a6c4145f6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = 'categorical_crossentropy'\n",
    "model.compile(loss=loss_function, optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a2ec628-04ac-4229-a2c8-509c7f6cff62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x1ebcb721dc0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6409fe07-d9e8-440f-a5bb-ce9ed3d5d4d7",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2760af39-9923-4671-8de5-012a2db00f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "70/70 [==============================] - 8s 30ms/step - loss: 1.2740 - accuracy: 0.1566\n",
      "Epoch 2/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 1.1534 - accuracy: 0.1694\n",
      "Epoch 3/20\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 1.1026 - accuracy: 0.1703\n",
      "Epoch 4/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 1.0395 - accuracy: 0.1739\n",
      "Epoch 5/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.9968 - accuracy: 0.1789\n",
      "Epoch 6/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.9447 - accuracy: 0.1862\n",
      "Epoch 7/20\n",
      "70/70 [==============================] - 2s 31ms/step - loss: 0.8987 - accuracy: 0.1947\n",
      "Epoch 8/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.8620 - accuracy: 0.2047\n",
      "Epoch 9/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.8236 - accuracy: 0.2146\n",
      "Epoch 10/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.7885 - accuracy: 0.2290\n",
      "Epoch 11/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.7528 - accuracy: 0.2456\n",
      "Epoch 12/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.7178 - accuracy: 0.2633\n",
      "Epoch 13/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.6894 - accuracy: 0.2854\n",
      "Epoch 14/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.6575 - accuracy: 0.3029\n",
      "Epoch 15/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.6296 - accuracy: 0.3330\n",
      "Epoch 16/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.5965 - accuracy: 0.3640\n",
      "Epoch 17/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.5640 - accuracy: 0.3937\n",
      "Epoch 18/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.5364 - accuracy: 0.4243\n",
      "Epoch 19/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.5088 - accuracy: 0.4556\n",
      "Epoch 20/20\n",
      "70/70 [==============================] - 2s 30ms/step - loss: 0.4794 - accuracy: 0.4872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ebd434dca0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%capture\n",
    "model.fit(generatorBatch(X_train, y_train, batch_size=batch_size), steps_per_epoch=len(X_train)//batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc91ca-861d-493a-8410-8c8e0bf75e11",
   "metadata": {},
   "source": [
    "# Предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c472e666-472f-41c1-a612-e4030b403419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_seq, encoder_model, decoder_model, tokenizer_output, reverse_word_map_target):\n",
    "    state_values_encoder = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tokenizer_output.word_index[start_target]\n",
    "    stop_condition = False\n",
    "    decoder_sentance = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        sample_word , decoder_h, decoder_c= decoder_model.predict([target_seq] + state_values_encoder)\n",
    "        sample_word_index = np.argmax(sample_word[0,-1,:])\n",
    "        decoder_word = reverse_word_map_target[sample_word_index]\n",
    "        decoder_sentance += ' ' + decoder_word\n",
    "        if (decoder_word == end_target or \n",
    "            len(decoder_sentance) > 70):\n",
    "            stop_condition = True\n",
    "        target_seq[0, 0] = sample_word_index\n",
    "        state_values_encoder = [decoder_h,decoder_c]\n",
    "    return decoder_sentance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530156f-4eb4-4639-9188-6e3a5f2868e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e9a3003-5b57-4ff3-9662-356141b5ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Encoder.getModel(model)\n",
    "decoder_model = Decoder.getModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3d3e888-63d2-4004-931e-e9183fec8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map_input  = dict(map(reversed, tokenizer_input.word_index.items()))\n",
    "reverse_word_map_target = dict(map(reversed, tokenizer_output.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99ee0b5e-9bc3-4102-851f-3b0f80afc344",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"D:\\Programms\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"D:\\Programms\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Programms\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"D:\\Programms\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"D:\\Programms\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"D:\\Programms\\Anaconda\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"Decoder\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 1, 256) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ONIGAT~1\\AppData\\Local\\Temp/ipykernel_38896/2965891324.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentance\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpad_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mpredicted_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse_word_map_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mreference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mhypothetic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ONIGAT~1\\AppData\\Local\\Temp/ipykernel_38896/1887997521.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(input_seq, encoder_model, decoder_model, tokenizer_output, reverse_word_map_target)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0msample_word\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdecoder_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_c\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstate_values_encoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0msample_word_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdecoder_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreverse_word_map_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_word_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programms\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"D:\\Programms\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"D:\\Programms\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Programms\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"D:\\Programms\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"D:\\Programms\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"D:\\Programms\\Anaconda\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"Decoder\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 1, 256) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    sentance = X_train[i]\n",
    "    original_target = y_train[i]\n",
    "    input_seq = tokenizer_input.texts_to_sequences([sentance])\n",
    "    pad_sequence = pad_sequences(input_seq, maxlen=30, padding='post')\n",
    "    predicted_target = predict(pad_sequence, encoder_model, decoder_model, tokenizer_output, reverse_word_map_target)\n",
    "    reference = original_target[5:-5].strip().split()\n",
    "    hypothetic = predicted_target[:-5].strip().split()\n",
    "    \n",
    "    print(f'Test #{i + 1}')\n",
    "    table = pd.Series([sentance, \n",
    "                       original_target[5:-5].strip(), \n",
    "                       predicted_target[:-5].strip(), \n",
    "                       \"%.3f\" % sentence_bleu([hypothetic], reference, weights = [1])], \n",
    "                          index=['Sentance','Original','Predicted', 'BLEU'])\n",
    "    \n",
    "    print(table.to_string())\n",
    "    print(\"==\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef51dc10-c109-429e-9bfa-34ca38f348f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test #1\n",
      "Sentance              нет я работал у меня отпуск будет зимой\n",
      "Original     нæ кусгæ кæнын мæнæн ме нцойуагъд зымæджы уыдзæн\n",
      "Predicted                        æз дæр дæр дæр дæр нæ федтай\n",
      "BLEU                                                    0.125\n",
      "====================================================================================================\n",
      "Test #2\n",
      "Sentance     расскажите нам всё что знаете\n",
      "Original     радзурут нын алцыдæр цы зонут\n",
      "Predicted             нæ цы æрцыди дыл нæу\n",
      "BLEU                                 0.200\n",
      "====================================================================================================\n",
      "Test #3\n",
      "Sentance     у нас возле дома есть автобусная остановка\n",
      "Original            нæ хæдзары цур ис автобусы æрлæууæн\n",
      "Predicted                мæ фыд дыууæ нана уайтагъд ног\n",
      "BLEU                                              0.000\n",
      "====================================================================================================\n",
      "Test #4\n",
      "Sentance       он его нашёл\n",
      "Original     уый йæ ссардта\n",
      "Predicted         уый бон у\n",
      "BLEU                  0.333\n",
      "====================================================================================================\n",
      "Test #5\n",
      "Sentance        мои уши красные\n",
      "Original     мæ хъустæ сырx сты\n",
      "Predicted      дæ ном куыд алан\n",
      "BLEU                      0.000\n",
      "====================================================================================================\n",
      "Test #6\n",
      "Sentance      том ест корм для кошек\n",
      "Original     том хæры гæдиты хæринаг\n",
      "Predicted        том мæнæй том дыл и\n",
      "BLEU                           0.195\n",
      "====================================================================================================\n",
      "Test #7\n",
      "Sentance     все хорошо спасибо\n",
      "Original      бузныг ницы мын у\n",
      "Predicted         хорз хорз дæн\n",
      "BLEU                      0.000\n",
      "====================================================================================================\n",
      "Test #8\n",
      "Sentance     кушать\n",
      "Original      хæрын\n",
      "Predicted       фос\n",
      "BLEU          0.000\n",
      "====================================================================================================\n",
      "Test #9\n",
      "Sentance        мы умеем переводить\n",
      "Original     мах тæлмац кæнын зонæм\n",
      "Predicted           дæ мыггаг кæмæй\n",
      "BLEU                          0.000\n",
      "====================================================================================================\n",
      "Test #10\n",
      "Sentance        завтра надо идти на работу\n",
      "Original     райсом мæ куыстмæ цæуын хъæуы\n",
      "Predicted                кæй цы кодтай нæу\n",
      "BLEU                                 0.000\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    sentance = X_test[i]\n",
    "    original_target = y_test[i]\n",
    "    input_seq = tokenizer_input.texts_to_sequences([sentance])\n",
    "    pad_sequence = pad_sequences(input_seq, maxlen= 30, padding='post')\n",
    "    predicted_target = predict(pad_sequence, encoder_model, decoder_model, tokenizer_output, reverse_word_map_target)\n",
    "    reference = original_target[5:-5].strip().split()\n",
    "    hypothetic = predicted_target[:-5].strip().split()\n",
    "    \n",
    "    print(f'Test #{i + 1}')\n",
    "    table = pd.Series([sentance, \n",
    "                       original_target[5:-5].strip(), \n",
    "                       predicted_target[:-5].strip(), \n",
    "                       \"%.3f\" % sentence_bleu([hypothetic], reference, weights = [1])], \n",
    "                          index=['Sentance','Original','Predicted', 'BLEU'])\n",
    "    \n",
    "    print(table.to_string())\n",
    "    print(\"==\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ef79a-1473-4761-ab31-760fdce76491",
   "metadata": {},
   "source": [
    "# Сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5db92bf9-7ce1-4119-9c7c-86a73a983319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizers(path, tokenizer_input, tokenizer_output):\n",
    "    with open(f'{path[5:-4]}-tokenizer_input.pkl','wb') as f:\n",
    "        pkl.dump(tokenizer_input, f)\n",
    "\n",
    "    with open(f'{path[5:-4]}-tokenizer_output.pkl','wb') as f:\n",
    "        pkl.dump(tokenizer_output, f)\n",
    "\n",
    "    pkl.dump(tokenizer_input, open(f'{path[5:-4]}-tokenizer_input.pkl', 'wb'))\n",
    "    pkl.dump(tokenizer_output, open(f'{path[5:-4]}-tokenizer_output.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55be3df3-8cf5-45af-ad79-bd480c2813a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_weights(model):\n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    with open(f'GRU-[{path[5:-4]}]-[Epochs={epochs}]-[LossFunction={loss_function}].json', \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    model.save_weights(f'GRU-[{path[5:-4]}]-[Epochs={epochs}]-[LossFunction={loss_function}]-[weight].h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c77ee5e-6a9d-4e78-8bca-fd2b90dd9910",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tokenizers(path, tokenizer_input, tokenizer_output)\n",
    "save_model_and_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0fdc9c-c067-483d-b87d-17aeb7c43e0b",
   "metadata": {},
   "source": [
    "# Загрузка и проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f60f5523-c7d3-4d9a-8419-4e2adcaa0ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_weights():\n",
    "    json_file = open(f'GRU-[{path[5:-4]}]-[Epochs={epochs}]-[LossFunction={loss_function}].json')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_loaded = model_from_json(loaded_model_json)\n",
    "\n",
    "    model_loaded.load_weights(f'GRU-[{path[5:-4]}]-[Epochs={epochs}]-[LossFunction={loss_function}]-[weight].h5')\n",
    "    \n",
    "    return model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7be24234-d0bd-403d-ad51-2852916e5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizers():\n",
    "    with open(f'{path[5:-4]}-tokenizer_input.pkl','rb') as f:\n",
    "        tokenizer_input = pkl.load(f)\n",
    "    with open(f'{path[5:-4]}-tokenizer_output.pkl','rb') as f:\n",
    "        tokenizer_output = pkl.load(f)\n",
    "\n",
    "    return tokenizer_input, tokenizer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9446ffd-4dd7-4cc4-8427-cbdfa5b70c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load = load_model_and_weights()\n",
    "tokenizer_input_load, tokenizer_output_load = load_tokenizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f20e6c3-8f86-454f-9347-ad6cd16ea51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_load = Encoder.getModel(model_load)\n",
    "decoder_model_load = Decoder.getModel(model_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2650e1a-26d5-4860-8722-9698abdf8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map_input  = dict(map(reversed, tokenizer_input_load.word_index.items()))\n",
    "reverse_word_map_target = dict(map(reversed, tokenizer_output_load.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80996037-4a32-4971-b873-ce8c3f6616a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test #1\n",
      "Sentance              нет я работал у меня отпуск будет зимой\n",
      "Original     нæ кусгæ кæнын мæнæн ме нцойуагъд зымæджы уыдзæн\n",
      "Predicted                        æз дæр дæр дæр дæр нæ федтай\n",
      "BLEU                                                    0.125\n",
      "====================================================================================================\n",
      "Test #2\n",
      "Sentance     расскажите нам всё что знаете\n",
      "Original     радзурут нын алцыдæр цы зонут\n",
      "Predicted             нæ цы æрцыди дыл нæу\n",
      "BLEU                                 0.200\n",
      "====================================================================================================\n",
      "Test #3\n",
      "Sentance     у нас возле дома есть автобусная остановка\n",
      "Original            нæ хæдзары цур ис автобусы æрлæууæн\n",
      "Predicted                мæ фыд дыууæ нана уайтагъд ног\n",
      "BLEU                                              0.000\n",
      "====================================================================================================\n",
      "Test #4\n",
      "Sentance       он его нашёл\n",
      "Original     уый йæ ссардта\n",
      "Predicted         уый бон у\n",
      "BLEU                  0.333\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    sentance = X_test[i]\n",
    "    original_target = y_test[i]\n",
    "    input_seq = tokenizer_input.texts_to_sequences([sentance])\n",
    "    pad_sequence = pad_sequences(input_seq, maxlen=30, padding='post')\n",
    "    predicted_target = predict(pad_sequence, encoder_model_load, decoder_model_load, tokenizer_output_load, reverse_word_map_target)\n",
    "    reference = original_target[5:-5].strip().split()\n",
    "    hypothetic = predicted_target[:-5].strip().split()\n",
    "    \n",
    "    print(f'Test #{i + 1}')\n",
    "    table = pd.Series([sentance, \n",
    "                       original_target[5:-5].strip(), \n",
    "                       predicted_target[:-5].strip(), \n",
    "                       \"%.3f\" % sentence_bleu([hypothetic], reference, weights = [1])], \n",
    "                          index=['Sentance','Original','Predicted', 'BLEU'])\n",
    "    \n",
    "    print(table.to_string())\n",
    "    print(\"==\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
