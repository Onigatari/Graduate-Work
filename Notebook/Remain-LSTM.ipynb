{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3bd3b4-219b-4c3b-a7ee-61d253092206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re, os, time, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle as pkl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import LSTM, GRU, Input, Dense, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d99dca-d5f9-47c8-bb27-2cd56b9b6926",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1337\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2628a3b-41a1-4f5a-ae0b-f0b86b334101",
   "metadata": {},
   "source": [
    "# 1. Подготовка даных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ddd6f5b-f224-40b9-bf48-917cb4828503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w, punctuation=False, toSeq=False):\n",
    "    \"\"\"\n",
    "        Функция для предобработки \n",
    "    \"\"\"\n",
    "    \n",
    "    # Уменьшаем регистр и убираем лишние пробелы\n",
    "    w = w.lower().strip()\n",
    "    \n",
    "    # Замена всех символов 'æ' на однотипный\n",
    "    w = re.sub(r\"ӕ\", r\"æ\", w)\n",
    "    \n",
    "    # Удаление апострофом\n",
    "    w = re.sub(\"'\", '', w)\n",
    "    \n",
    "    if punctuation:\n",
    "        # Делаем между словом и знаком пунктуации отступ 'слово! -> слово !'\n",
    "        w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    else:\n",
    "        # Удаляет все знаки пунктуации\n",
    "        w = re.sub(r\"[^\\w\\s]\", r\"\", w)\n",
    "    \n",
    "    # Выкидываем все остальные символы из рассмотрения \n",
    "    w = re.sub(r\"[^a-яА-Яa-zA-Z?.!,æё]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # Добавляем токены для начала и конца предложения\n",
    "    if toSeq:\n",
    "        w = f'<sos> {w} <eos>'\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06889a21-0434-4753-883e-bf24d25b3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "        Функция, которая создаёт датасет\n",
    "    \"\"\"    \n",
    "    new_path = f'Date/{path}'\n",
    "    with open(new_path, encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    uncleaned_data_list = data.split('\\n')\n",
    "    \n",
    "    source_word = []\n",
    "    target_word = []\n",
    "    for word in uncleaned_data_list:\n",
    "        source_word.append(preprocess_sentence(word.split('\\t')[0], punctuation=False, toSeq=False))\n",
    "        target_word.append(preprocess_sentence(word.split('\\t')[1], punctuation=False, toSeq=True))\n",
    "        \n",
    "    language_data = pd.DataFrame(columns=['Source','Target'])\n",
    "    language_data['Source'] = source_word\n",
    "    language_data['Target'] = target_word\n",
    "    \n",
    "    return language_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a3d01f-6287-43ee-8672-526b327e7dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_(text_data):\n",
    "    '''\n",
    "        Токенайзер\n",
    "    '''\n",
    "    \n",
    "    tokenizer = Tokenizer(filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(text_data)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8401d9-c9c7-402a-a4f5-4b363fe5464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(data):\n",
    "    max_length_ = max([len(x.split(' ')) for x in data])\n",
    "    return max_length_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "298ea3a1-6e0b-4994-b6be-3e235160b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preparing_data(input_seq, output_seq):\n",
    "    tokenizer_input, tokenizer_output = tokenizer_(input_seq), tokenizer_(output_seq)\n",
    "    input_max_length, output_max_length = len(tokenizer_input.word_index) + 1, len(tokenizer_output.word_index) + 1\n",
    "    \n",
    "    return tokenizer_input, tokenizer_output, input_max_length, output_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138a1e2-7510-4933-b3f1-59d31d2efd0b",
   "metadata": {},
   "source": [
    "# 2. Построение модели Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b68f6f-6c03-4d2a-8828-b99c366888d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "        Энкодер \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size_input, HIDDEN_DIM):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.inputs = Input(shape=(None,), name=\"encoder_inputs\")\n",
    "        self.embedding = Embedding(vocab_size_input, HIDDEN_DIM, mask_zero=True, name=\"encoder_embedding\")(self.inputs)\n",
    "        \n",
    "        encoder = LSTM(HIDDEN_DIM, return_state=True, name=\"encoder_lstm\")\n",
    "        self.outputs, state_h, state_c = encoder(self.embedding)\n",
    "        self.states = [state_h, state_c]\n",
    "        \n",
    "def getEncoder(model_loaded):\n",
    "    encoder_inputs_inf = model_loaded.input[0]\n",
    "    encoder_outputs_inf, inf_state_h, inf_state_c = model_loaded.layers[4].output\n",
    "    encoder_inf_states = [inf_state_h,inf_state_c]\n",
    "\n",
    "    return Model(encoder_inputs_inf, encoder_inf_states, name='Encoder') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "454a1f31-5c8f-4274-9b7e-b6a712e855a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "        Декодер \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size_output, HIDDEN_DIM, encoder_states):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.inputs = Input(shape=(None,), name=\"decoder_inputs\")\n",
    "        self.embedding = Embedding(vocab_size_output, HIDDEN_DIM, mask_zero=True, name=\"decoder_embedding\")(self.inputs)\n",
    "        \n",
    "        decoder = LSTM(HIDDEN_DIM, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "        self.outputs, _, _ = decoder(self.embedding, initial_state=encoder_states)\n",
    "        self.dense = Dense(vocab_size_output, activation='softmax', name=\"dense_lstm\")\n",
    "        self.outputs = self.dense(self.outputs)\n",
    "        \n",
    "def getDecoder(model_loaded):\n",
    "    decoder_state_h_input = Input(shape=(HIDDEN_DIM,))\n",
    "    decoder_state_c_input = Input(shape=(HIDDEN_DIM,))\n",
    "    decoder_state_input = [decoder_state_h_input,decoder_state_c_input]\n",
    "\n",
    "    decoder_input_inf = model_loaded.input[1]\n",
    "    decoder_emb_inf = model_loaded.layers[3](decoder_input_inf)\n",
    "    decoder_lstm_inf = model_loaded.layers[5]\n",
    "    decoder_output_inf, decoder_state_h_inf, decoder_state_c_inf = decoder_lstm_inf(decoder_emb_inf, initial_state=decoder_state_input)\n",
    "    decoder_state_inf = [decoder_state_h_inf, decoder_state_c_inf]\n",
    "    dense_inf = model_loaded.layers[6]\n",
    "    decoder_output_final = dense_inf(decoder_output_inf)\n",
    "\n",
    "    return Model([decoder_input_inf]+decoder_state_input, [decoder_output_final]+decoder_state_inf, name='Decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81ad8a6a-e1dc-48de-b446-c2d7ff443ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Batch(X, Y, batch_size):\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_data_input = np.zeros((batch_size,max_lenght_source),dtype='float32') #metrix of batch_size*max_length_english\n",
    "            decoder_data_input = np.zeros((batch_size,max_lenght_target),dtype='float32') #metrix of batch_size*max_length_marathi\n",
    "            decoder_target_input = np.zeros((batch_size,max_lenght_target,vocab_size_target),dtype='float32') # 3d array one hot encoder decoder target data\n",
    "            for i, (input_text,target_text) in enumerate(zip(X[j:j+batch_size],Y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_data_input[i,t] = tokenizer_input.word_index[word] # Here we are storing the encoder \n",
    "                                                                         #seq in row here padding is done automaticaly as \n",
    "                                                                         #we have defined col as max_lenght\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    decoder_data_input[i,t] = tokenizer_output.word_index[word] # same for the decoder sequence\n",
    "                    if t>0:\n",
    "                        decoder_target_input[i,t-1,tokenizer_output.word_index[word]] = 1 #target is one timestep ahead of decoder input because it does not have 'start tag'\n",
    "            yield ([encoder_data_input,decoder_data_input],decoder_target_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a70c8d-84ee-44a6-ba55-6a17657e615d",
   "metadata": {},
   "source": [
    "# 3. Входные параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f10c3abb-6cec-444f-b8fc-dd3b5c769a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 50\n",
    "batch_size = 6\n",
    "epochs = 20\n",
    "\n",
    "start_target = \"<sos>\"\n",
    "end_target = \"<eos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e15cd9ae-e669-4474-9ea3-86744e9fb55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'rus-oss.txt'\n",
    "data = load_dataset(path)\n",
    "input_seq, output_seq = data['Source'].values, data['Target'].values\n",
    "tokenizer_input, tokenizer_output, vocab_size_source, vocab_size_target = get_preparing_data(input_seq, output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d268c4ac-2e11-40ab-bb2a-7e59897bc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path[:-4]}-tokenizer_input.pkl','wb') as f:\n",
    "    pkl.dump(tokenizer_input, f)\n",
    "\n",
    "with open(f'{path[:-4]}-tokenizer_output.pkl','wb') as f:\n",
    "    pkl.dump(tokenizer_output, f)\n",
    "\n",
    "pkl.dump(tokenizer_input, open(f'{path[:-4]}-tokenizer_input.pkl', 'wb'))\n",
    "pkl.dump(tokenizer_output, open(f'{path[:-4]}-tokenizer_output.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc0848b-e173-4914-84c0-aef68b3845cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_seq, output_seq, test_size = 0.1)\n",
    "train_samples = len(X_train)\n",
    "test_samples = len(X_test)\n",
    "\n",
    "max_lenght_source = max_length(X_train)\n",
    "max_lenght_target = max_length(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8fb90c3-e972-4fa3-94ca-0b3c1067b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size_source, HIDDEN_DIM)\n",
    "decoder = Decoder(vocab_size_target, HIDDEN_DIM, encoder.states)\n",
    "\n",
    "model = Model([encoder.inputs, decoder.inputs], decoder.outputs, name=\"LSTM-Translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c28ea3a-7a69-4e5b-a24a-a6c4145f6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = [\n",
    "    'categorical_crossentropy',\n",
    "    'binary_crossentropy',\n",
    "]\n",
    "\n",
    "now_loss_function = loss_function[0]\n",
    "model.compile(loss=now_loss_function, optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6409fe07-d9e8-440f-a5bb-ce9ed3d5d4d7",
   "metadata": {},
   "source": [
    "# 4. Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2760af39-9923-4671-8de5-012a2db00f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ONIGAT~1\\AppData\\Local\\Temp/ipykernel_19056/1321299230.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator = Batch(X_train, y_train, batch_size = batch_size), steps_per_epoch = train_samples//batch_size, epochs=epochs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 7s 17ms/step - loss: 1.3157 - accuracy: 0.1543\n",
      "Epoch 2/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.1914 - accuracy: 0.1577\n",
      "Epoch 3/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.1817 - accuracy: 0.1571\n",
      "Epoch 4/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.1539 - accuracy: 0.1583\n",
      "Epoch 5/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.1394 - accuracy: 0.1616\n",
      "Epoch 6/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.1245 - accuracy: 0.1681\n",
      "Epoch 7/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.1066 - accuracy: 0.1698\n",
      "Epoch 8/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.0946 - accuracy: 0.1696\n",
      "Epoch 9/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.0788 - accuracy: 0.1708\n",
      "Epoch 10/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.0609 - accuracy: 0.1709\n",
      "Epoch 11/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.0414 - accuracy: 0.1708\n",
      "Epoch 12/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.0219 - accuracy: 0.1729\n",
      "Epoch 13/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 1.0077 - accuracy: 0.1759\n",
      "Epoch 14/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 0.9890 - accuracy: 0.1780\n",
      "Epoch 15/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 0.9786 - accuracy: 0.1776\n",
      "Epoch 16/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 0.9603 - accuracy: 0.1813\n",
      "Epoch 17/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 0.9484 - accuracy: 0.1840\n",
      "Epoch 18/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 0.9361 - accuracy: 0.1859\n",
      "Epoch 19/20\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 0.9242 - accuracy: 0.1879\n",
      "Epoch 20/20\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 0.9190 - accuracy: 0.1866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f3beb61a90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%capture\n",
    "model.fit_generator(generator = Batch(X_train, y_train, batch_size = batch_size), steps_per_epoch = train_samples//batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f6a63-4868-4380-a9ec-3ea3069b8192",
   "metadata": {},
   "source": [
    "# 5. Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ebbda12-686d-47de-af07-63661bd10b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отрисовка схемы модели\n",
    "# plot_model(model, to_file=f'{dir}-{epochs}-train_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7db8682d-539a-4ace-9c95-3f414ee4fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save_JSON():\n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    with open(f'LSTM-[{path[:-4]}]-[Epochs={epochs}]-[LossFunction={now_loss_function}].json', \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    model.save_weights(f'LSTM-[{path[:-4]}]-[Epochs={epochs}]-[LossFunction={now_loss_function}]-[weight].h5')\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a270dfdd-590f-46f9-88ac-331d15f8b34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_save_JSON()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa48a794-6ae1-4ac4-bf9a-c57c1fa1918e",
   "metadata": {},
   "source": [
    "# 6. Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7155f866-a36c-4996-a56b-ee5682ae6e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load_JSON():\n",
    "    json_file = open(f'LSTM-[{path[:-4]}]-[Epochs={epochs}]-[LossFunction={now_loss_function}].json')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_loaded = model_from_json(loaded_model_json)\n",
    "\n",
    "    model_loaded.load_weights(f'LSTM-[{path[:-4]}]-[Epochs={epochs}]-[LossFunction={now_loss_function}]-[weight].h5')\n",
    "    print(\"Model loaded\")\n",
    "    \n",
    "    return model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd55a3e8-a44f-4df8-b659-eb8d10788911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model_loaded = model_load_JSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dbc917e-9bdb-4b2d-993d-209fc0eb5a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM-Translation\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " encoder_embedding (Embedding)  (None, None, 50)     47250       ['encoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " decoder_embedding (Embedding)  (None, None, 50)     46100       ['decoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)            [(None, 50),         20200       ['encoder_embedding[0][0]']      \n",
      "                                 (None, 50),                                                      \n",
      "                                 (None, 50)]                                                      \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 50),   20200       ['decoder_embedding[0][0]',      \n",
      "                                 (None, 50),                      'encoder_lstm[0][1]',           \n",
      "                                 (None, 50)]                      'encoder_lstm[0][2]']           \n",
      "                                                                                                  \n",
      " dense_lstm (Dense)             (None, None, 922)    47022       ['decoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 180,772\n",
      "Trainable params: 180,772\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba7c15b6-9095-453e-abb7-465a7504273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = getEncoder(model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28f3ab23-a88c-47ab-8791-53785ae80f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = getDecoder(model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d118094-3de5-4929-8071-403404efc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path[:-4]}-tokenizer_input.pkl','rb') as f:\n",
    "    tokenizer_input = pkl.load(f)\n",
    "with open(f'{path[:-4]}-tokenizer_output.pkl','rb') as f:\n",
    "    tokenizer_output = pkl.load(f)\n",
    "\n",
    "reverse_word_map_input = dict(map(reversed, tokenizer_input.word_index.items()))\n",
    "reverse_word_map_target = dict(map(reversed, tokenizer_output.word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530156f-4eb4-4639-9188-6e3a5f2868e5",
   "metadata": {},
   "source": [
    "# 7. Проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2c2f350-557d-4264-bc12-0f1d28d0f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(input_seq):\n",
    "    state_values_encoder = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tokenizer_output.word_index[start_target]\n",
    "    stop_condition = False\n",
    "    decoder_sentance = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        sample_word , decoder_h,decoder_c= decoder_model.predict([target_seq] + state_values_encoder)\n",
    "        sample_word_index = np.argmax(sample_word[0,-1,:])\n",
    "        decoder_word = reverse_word_map_target[sample_word_index]\n",
    "        decoder_sentance += ' '+ decoder_word\n",
    "        if (decoder_word == end_target or \n",
    "            len(decoder_sentance) > 70):\n",
    "            stop_condition = True\n",
    "        target_seq[0, 0] = sample_word_index\n",
    "        state_values_encoder = [decoder_h,decoder_c]\n",
    "    return decoder_sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef51dc10-c109-429e-9bfa-34ca38f348f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[array([[ 0.9784169 , -0.19485119, -0.6122091 , -0.64868265, -0.7230219 ,\n",
      "         0.8106158 ,  0.8107648 ,  0.91934127,  0.72031766,  0.8157941 ,\n",
      "        -0.8585522 , -0.7003038 , -0.602488  , -0.81738394, -0.8224007 ,\n",
      "         0.85457194, -0.34258518, -0.59603447,  0.8486576 , -0.9538733 ,\n",
      "         0.8287809 , -0.92376494, -0.96933573, -0.7721541 , -0.9891983 ,\n",
      "        -0.8951807 ,  0.592871  , -0.9265781 , -0.08316586, -0.6631687 ,\n",
      "        -0.7029446 , -0.7334977 ,  0.7473687 , -0.7191986 , -0.87851393,\n",
      "        -0.86956626, -0.9565901 ,  0.7424064 ,  0.86893684, -0.92465353,\n",
      "        -0.92020404, -0.7423994 ,  0.71024054,  0.9030272 ,  0.7706374 ,\n",
      "         0.90187925,  0.95504624, -0.69024986, -0.95539093, -0.9932902 ]],\n",
      "      dtype=float32), array([[ 5.9866495 , -4.1878066 , -1.894134  , -4.96538   , -3.521703  ,\n",
      "         1.1391537 ,  2.5970612 ,  3.0193996 ,  1.6579196 ,  3.1233358 ,\n",
      "        -2.4779065 , -1.7889435 , -1.6550264 , -2.4661868 , -3.4559746 ,\n",
      "         2.79838   , -0.49632093, -1.0603048 ,  4.361244  , -5.0486946 ,\n",
      "         1.3245088 , -4.8555546 , -4.5816803 , -1.3934848 , -5.2173243 ,\n",
      "        -3.9987411 ,  4.0765114 , -4.5139236 , -0.14729121, -5.2245817 ,\n",
      "        -1.592433  , -3.218114  ,  3.337087  , -3.037331  , -2.8521724 ,\n",
      "        -4.83202   , -4.5437355 ,  3.7791917 ,  1.4561906 , -3.9621944 ,\n",
      "        -4.311566  , -2.710814  ,  4.217623  ,  5.2701235 ,  3.404601  ,\n",
      "         5.3987966 ,  5.320438  , -5.961817  , -3.962972  , -3.4358916 ]],\n",
      "      dtype=float32)]\n",
      "Test sentance:  1\n",
      "sentance:  нет я работал у меня отпуск будет зимой\n",
      "origianl translate:  нæ кусгæ кæнын мæнæн ме нцойуагъд зымæджы уыдзæн \n",
      "predicted Translate:  æз æз нæ фæнды нæ фæнды у \n",
      "====================================================================================================\n",
      "\n",
      "[array([[ 0.9039528 , -0.42133144, -0.5897319 , -0.6959186 , -0.15783486,\n",
      "         0.6926754 ,  0.5255439 ,  0.27391353,  0.46325743,  0.6460031 ,\n",
      "        -0.17624189, -0.55990636, -0.6803679 , -0.3479711 , -0.4834977 ,\n",
      "         0.5860213 , -0.23935342,  0.21236897,  0.6343634 , -0.87788254,\n",
      "         0.22435214, -0.8244729 , -0.84789044, -0.03661072, -0.6317374 ,\n",
      "        -0.742325  ,  0.02129005, -0.7415788 ,  0.13370049, -0.65016955,\n",
      "        -0.33354583, -0.696952  ,  0.20107953, -0.16426711, -0.30694973,\n",
      "        -0.5926412 , -0.74865407,  0.738284  ,  0.45936042, -0.79490626,\n",
      "        -0.7984635 ,  0.03480491,  0.69517416,  0.51810527,  0.24682583,\n",
      "         0.7653251 ,  0.5451157 , -0.7430845 , -0.8267384 , -0.8748079 ]],\n",
      "      dtype=float32), array([[ 2.2061975 , -1.2303727 , -1.4578998 , -2.197381  , -0.2811051 ,\n",
      "         0.9683735 ,  0.9385382 ,  0.36213192,  0.7656585 ,  1.5693998 ,\n",
      "        -0.24518088, -1.0600985 , -1.5677233 , -0.51878214, -0.7761671 ,\n",
      "         1.0083058 , -0.37618983,  0.36264557,  1.1278695 , -2.068335  ,\n",
      "         0.26165944, -2.4354975 , -1.8387146 , -0.04970143, -0.8661598 ,\n",
      "        -1.4451554 ,  0.03468961, -1.6794856 ,  0.20160383, -2.3104274 ,\n",
      "        -0.5683552 , -1.6257682 ,  0.3152308 , -0.26199663, -0.41946474,\n",
      "        -1.3554394 , -1.6172116 ,  1.8341057 ,  0.5931435 , -2.005634  ,\n",
      "        -1.8499341 ,  0.05475692,  1.417135  ,  0.796741  ,  0.38024577,\n",
      "         1.3754464 ,  0.7774058 , -2.1492882 , -1.9231966 , -1.7391374 ]],\n",
      "      dtype=float32)]\n",
      "Test sentance:  2\n",
      "sentance:  расскажите нам всё что знаете\n",
      "origianl translate:  радзурут нын алцыдæр цы зонут \n",
      "predicted Translate:  мæ нæ кодтай \n",
      "====================================================================================================\n",
      "\n",
      "[array([[ 0.96277535, -0.26822475, -0.594762  , -0.6660535 , -0.5875065 ,\n",
      "         0.6659917 ,  0.7238515 ,  0.38853303,  0.58587736,  0.7829892 ,\n",
      "        -0.24798557, -0.6334713 , -0.6728964 , -0.779211  , -0.77247995,\n",
      "         0.7663874 , -0.01221782,  0.5354195 ,  0.75052434, -0.9454443 ,\n",
      "        -0.7266523 , -0.8648839 , -0.94462585,  0.44120368, -0.9551597 ,\n",
      "        -0.8748905 ,  0.2407876 , -0.83013016, -0.4599366 , -0.622354  ,\n",
      "        -0.53201234, -0.7855313 ,  0.5991935 , -0.5594782 , -0.6451951 ,\n",
      "        -0.7441503 , -0.9170259 ,  0.7963205 ,  0.93764895, -0.86308044,\n",
      "        -0.8781354 , -0.50898165,  0.7789643 ,  0.8590283 ,  0.6206896 ,\n",
      "         0.8924699 ,  0.85050714, -0.66468734, -0.8972168 , -0.95018476]],\n",
      "      dtype=float32), array([[ 4.485928  , -2.4502811 , -2.3365247 , -3.9894757 , -1.4656401 ,\n",
      "         0.8248819 ,  1.7568264 ,  0.47285038,  1.2028041 ,  2.2388911 ,\n",
      "        -0.31135982, -1.6163586 , -2.4287667 , -2.209335  , -2.0836277 ,\n",
      "         1.6236871 , -0.01831177,  1.1909509 ,  2.595236  , -3.8508224 ,\n",
      "        -1.1750014 , -4.094516  , -3.6794748 ,  0.63604945, -2.5466146 ,\n",
      "        -2.7396877 ,  0.5424409 , -3.504787  , -1.5415033 , -4.148578  ,\n",
      "        -1.0293245 , -2.7622786 ,  1.365345  , -1.2148674 , -1.082516  ,\n",
      "        -3.5231595 , -3.3986597 ,  3.4011471 ,  2.386473  , -3.2784925 ,\n",
      "        -3.0800247 , -0.86898273,  3.1320484 ,  3.1085906 ,  1.1282549 ,\n",
      "         3.3496165 ,  2.075685  , -4.269414  , -3.0778327 , -2.131206  ]],\n",
      "      dtype=float32)]\n",
      "Test sentance:  3\n",
      "sentance:  у нас возле дома есть автобусная остановка\n",
      "origianl translate:  нæ хæдзары цур ис автобусы æрлæууæн \n",
      "predicted Translate:  мæ йæ йæ ис йæ кодтай йæ \n",
      "====================================================================================================\n",
      "\n",
      "[array([[ 0.85371304, -0.34166077, -0.33573654, -0.52447885, -0.3934942 ,\n",
      "         0.757197  ,  0.3441057 ,  0.45256025,  0.46088806,  0.6102637 ,\n",
      "        -0.33096385, -0.52024114, -0.48698953, -0.24903141, -0.41772732,\n",
      "         0.51089954, -0.1694171 , -0.10733312,  0.60176045, -0.81445783,\n",
      "         0.46631065, -0.7357852 , -0.7913868 , -0.2768186 , -0.79398763,\n",
      "        -0.5345931 ,  0.37137926, -0.76277316,  0.26723626, -0.52336764,\n",
      "        -0.19524805, -0.50383157,  0.400044  , -0.27979255, -0.46962148,\n",
      "        -0.6905671 , -0.7467064 ,  0.58538324,  0.12912162, -0.7106636 ,\n",
      "        -0.71052206, -0.19051678,  0.54953086,  0.66809416,  0.33519316,\n",
      "         0.73410666,  0.6953524 , -0.5987523 , -0.7597315 , -0.84966147]],\n",
      "      dtype=float32), array([[ 1.8503572 , -1.3047478 , -0.7013241 , -1.402561  , -0.7798283 ,\n",
      "         1.1022558 ,  0.5627407 ,  0.6495113 ,  0.84089303,  1.3071165 ,\n",
      "        -0.48697904, -1.1433897 , -1.0704381 , -0.35373095, -0.64582753,\n",
      "         0.83838385, -0.28067353, -0.17437413,  1.1780901 , -1.7493892 ,\n",
      "         0.6005773 , -1.5755    , -1.5535301 , -0.38341597, -1.3135114 ,\n",
      "        -0.918627  ,  0.72462356, -1.5962763 ,  0.4793133 , -1.5849783 ,\n",
      "        -0.32799864, -1.0487479 ,  0.6887716 , -0.5222061 , -0.6911744 ,\n",
      "        -1.3557649 , -1.3669857 ,  1.4691042 ,  0.14843363, -1.379905  ,\n",
      "        -1.4330688 , -0.29614562,  1.2871047 ,  1.3214527 ,  0.5771702 ,\n",
      "         1.523784  ,  1.145751  , -1.7405767 , -1.3630083 , -1.4355316 ]],\n",
      "      dtype=float32)]\n",
      "Test sentance:  4\n",
      "sentance:  он его нашёл\n",
      "origianl translate:  уый йæ ссардта \n",
      "predicted Translate:  æз нæ \n",
      "====================================================================================================\n",
      "\n",
      "[array([[ 0.52772045, -0.14485651, -0.16371024, -0.34522602,  0.00878368,\n",
      "         0.5145057 ,  0.12244124, -0.11406478,  0.01313468,  0.3978093 ,\n",
      "         0.06982112, -0.10829271, -0.30072936, -0.2652646 , -0.24839573,\n",
      "         0.09675968,  0.08217615,  0.15047759,  0.02179053, -0.4984298 ,\n",
      "        -0.14569438, -0.42332026, -0.43567982,  0.14883795, -0.06997245,\n",
      "        -0.23753174, -0.08776207, -0.441153  , -0.09452676, -0.363267  ,\n",
      "         0.00406877, -0.30817932, -0.09922408,  0.07731104,  0.02434872,\n",
      "        -0.34176466, -0.53237873,  0.37645388,  0.31083813, -0.4044304 ,\n",
      "        -0.33368587,  0.10848752,  0.238337  ,  0.03380039, -0.06751826,\n",
      "         0.22201619, -0.02730566, -0.36182886, -0.49202225, -0.55807436]],\n",
      "      dtype=float32), array([[ 0.9956855 , -0.36902943, -0.40722853, -0.84097373,  0.01645142,\n",
      "         0.79791415,  0.22720477, -0.18920872,  0.02760512,  0.73847735,\n",
      "         0.11930656, -0.24882975, -0.6812613 , -0.45256522, -0.41904742,\n",
      "         0.16513462,  0.17368695,  0.3087286 ,  0.04109153, -0.98864454,\n",
      "        -0.2401044 , -0.9352976 , -0.8080934 ,  0.2670216 , -0.0968039 ,\n",
      "        -0.45439565, -0.18781951, -0.78574777, -0.2187357 , -0.9453976 ,\n",
      "         0.0086105 , -0.6013837 , -0.19269255,  0.16818309,  0.03578376,\n",
      "        -0.6145833 , -0.9632266 ,  0.8312148 ,  0.48186827, -0.780115  ,\n",
      "        -0.6632793 ,  0.18674304,  0.4895616 ,  0.05435444, -0.12458351,\n",
      "         0.38785493, -0.04434069, -0.9672507 , -0.88846916, -0.87137496]],\n",
      "      dtype=float32)]\n",
      "Test sentance:  5\n",
      "sentance:  мои уши красные\n",
      "origianl translate:  мæ хъустæ сырx сты \n",
      "predicted Translate:  уый йæ \n",
      "====================================================================================================\n",
      "\n",
      "[array([[ 0.90430486, -0.3083292 , -0.5070366 , -0.64435333, -0.07469313,\n",
      "         0.61483836,  0.5361803 , -0.3172552 ,  0.29107004,  0.68999934,\n",
      "         0.09015734, -0.46506238, -0.6467127 , -0.6127233 , -0.610036  ,\n",
      "         0.5183562 ,  0.08671766,  0.41116983,  0.52759403, -0.88069785,\n",
      "        -0.5552627 , -0.7694386 , -0.8626031 ,  0.40060943, -0.53751546,\n",
      "        -0.7288671 , -0.17869589, -0.73766965, -0.35943288, -0.5934838 ,\n",
      "        -0.2997977 , -0.69861233, -0.0147591 , -0.04814798, -0.24226266,\n",
      "        -0.5921209 , -0.80780584,  0.7295553 ,  0.78369135, -0.7514261 ,\n",
      "        -0.74563384,  0.05788524,  0.68468267,  0.62616855,  0.10739313,\n",
      "         0.7687967 ,  0.3090501 , -0.6213938 , -0.80752176, -0.8527853 ]],\n",
      "      dtype=float32), array([[ 2.821321  , -1.0861418 , -1.5622439 , -2.4901958 , -0.13224113,\n",
      "         0.80293465,  1.0200523 , -0.44696665,  0.5144263 ,  1.6977863 ,\n",
      "         0.12298881, -1.0089672 , -1.9349453 , -1.2480183 , -1.1844256 ,\n",
      "         0.87206066,  0.15190513,  0.9967972 ,  1.0116043 , -2.5549636 ,\n",
      "        -0.87202096, -2.6708496 , -2.3675501 ,  0.6867728 , -0.7111401 ,\n",
      "        -1.5419128 , -0.3895503 , -2.2652855 , -0.94096804, -2.6368423 ,\n",
      "        -0.58071136, -1.8931911 , -0.02473944, -0.0859323 , -0.34118807,\n",
      "        -1.939001  , -2.2180011 ,  2.2893355 ,  1.4400539 , -2.057876  ,\n",
      "        -1.9426242 ,  0.08672581,  1.729018  ,  1.0901735 ,  0.15645584,\n",
      "         1.6399376 ,  0.43144116, -2.617371  , -2.137042  , -1.5998597 ]],\n",
      "      dtype=float32)]\n",
      "Test sentance:  6\n",
      "sentance:  том ест корм для кошек\n",
      "origianl translate:  том хæры гæдиты хæринаг \n",
      "predicted Translate:  мæ йæ ис йæ кодтай йæ \n",
      "====================================================================================================\n",
      "\n",
      "[array([[ 0.96133566, -0.4577241 , -0.61484766, -0.66880125, -0.6403288 ,\n",
      "         0.82589376,  0.6823462 ,  0.89777815,  0.81128514,  0.6974432 ,\n",
      "        -0.7739024 , -0.7809907 , -0.6212767 ,  0.5712353 , -0.32335678,\n",
      "         0.8424895 , -0.7381097 , -0.7450882 ,  0.8935049 , -0.94490236,\n",
      "         0.95606995, -0.92032874, -0.93096125, -0.86845547, -0.96122104,\n",
      "        -0.8281488 ,  0.78287375, -0.9085412 ,  0.81751716, -0.6543157 ,\n",
      "        -0.6565046 , -0.65865105,  0.8146348 , -0.7167921 , -0.8403889 ,\n",
      "        -0.8777218 , -0.846763  ,  0.74552536, -0.87768686, -0.9115326 ,\n",
      "        -0.9212183 , -0.66240436,  0.736055  ,  0.85637   ,  0.7155119 ,\n",
      "         0.91825217,  0.958678  , -0.8229472 , -0.8995084 , -0.9608948 ]],\n",
      "      dtype=float32), array([[ 2.2323387, -2.074249 , -1.0792112, -1.7415681, -1.5733899,\n",
      "         1.1897581,  1.2062421,  1.9453049,  1.6181071,  1.7349522,\n",
      "        -1.465096 , -1.6789111, -1.1183553,  0.8299563, -0.4038238,\n",
      "         1.7967453, -1.494766 , -1.5812521,  2.1206658, -2.1736805,\n",
      "         2.1721053, -2.0159743, -1.9173483, -1.7349811, -2.147062 ,\n",
      "        -1.5691595,  1.9488769, -1.9540809,  2.0160239, -1.9834008,\n",
      "        -1.0573642, -1.4464784,  1.7549748, -1.6103404, -1.6642153,\n",
      "        -1.6883557, -1.4145287,  1.770242 , -1.5094993, -1.9752284,\n",
      "        -2.098594 , -1.3431443,  1.8801064,  2.0428615,  1.7007136,\n",
      "         2.168985 ,  2.3224916, -2.174508 , -1.687089 , -2.0379436]],\n",
      "      dtype=float32)]\n",
      "Test sentance:  7\n",
      "sentance:  все хорошо спасибо\n",
      "origianl translate:  бузныг ницы мын у \n",
      "predicted Translate:  æз хорз \n",
      "====================================================================================================\n",
      "\n",
      "[array([[ 5.5993881e-02, -3.1477712e-02, -7.4681334e-02, -8.0285631e-02,\n",
      "         8.4817298e-03,  1.4141569e-02,  4.5680605e-02, -2.1849554e-03,\n",
      "         5.1125349e-03,  1.1266309e-02,  8.8199839e-04, -6.9110398e-03,\n",
      "        -3.0829798e-02, -4.1090962e-03, -3.3267054e-03,  1.5361428e-02,\n",
      "        -3.1786975e-03, -8.9546469e-05,  2.0130677e-03, -5.2780788e-02,\n",
      "         1.2743549e-02, -7.9565562e-02, -3.7487529e-02,  9.3614071e-04,\n",
      "        -1.7873356e-02, -6.3222378e-02, -9.5236152e-03,  3.5197217e-02,\n",
      "         1.9287795e-02, -7.8858554e-02, -1.9243987e-02, -6.6715643e-02,\n",
      "        -1.3870267e-02,  3.4020355e-03,  1.2790486e-02,  4.4239338e-02,\n",
      "        -5.0042872e-03,  6.0379725e-02,  7.0343288e-03, -6.9537155e-02,\n",
      "        -5.4596949e-02,  6.0244757e-03,  2.9350558e-02, -1.8217528e-02,\n",
      "        -1.0729991e-02,  2.1317247e-03,  6.3409512e-03, -6.4269595e-02,\n",
      "        -7.2108135e-02, -2.3535425e-02]], dtype=float32), array([[ 0.10386282, -0.05902551, -0.1441925 , -0.15042667,  0.01644734,\n",
      "         0.02531398,  0.08663733, -0.00401882,  0.00952104,  0.02085908,\n",
      "         0.00165854, -0.01324082, -0.05817068, -0.00754877, -0.0061682 ,\n",
      "         0.02921625, -0.00593511, -0.00017148,  0.00380035, -0.09689029,\n",
      "         0.02328392, -0.14861102, -0.07045498,  0.00176635, -0.03251625,\n",
      "        -0.11899406, -0.0184349 ,  0.06427592,  0.03586761, -0.14602871,\n",
      "        -0.03684419, -0.12641011, -0.02622981,  0.00637312,  0.0231798 ,\n",
      "         0.07849753, -0.00912473,  0.11155096,  0.01278197, -0.12941565,\n",
      "        -0.10114518,  0.01135235,  0.0551811 , -0.03357401, -0.02014272,\n",
      "         0.0040373 ,  0.01205463, -0.12639388, -0.13428983, -0.04264775]],\n",
      "      dtype=float32)]\n",
      "Test sentance:  8\n",
      "sentance:  кушать\n",
      "origianl translate:  хæрын \n",
      "predicted Translate:  кæй \n",
      "====================================================================================================\n",
      "\n",
      "[array([[ 0.6704534 , -0.27647588, -0.28372967, -0.4607284 , -0.0918766 ,\n",
      "         0.55738264,  0.20583569,  0.07441017,  0.18173161,  0.44428915,\n",
      "        -0.0497232 , -0.2923055 , -0.4266832 , -0.19075163, -0.24445961,\n",
      "         0.26173034, -0.05096757,  0.06980447,  0.28449535, -0.6449274 ,\n",
      "         0.03319326, -0.56053865, -0.57623094,  0.03030965, -0.32639843,\n",
      "        -0.35669297,  0.03951158, -0.52673376,  0.05292653, -0.457676  ,\n",
      "        -0.09365383, -0.40216717,  0.03447481, -0.03584663, -0.09169085,\n",
      "        -0.39687815, -0.5356849 ,  0.49530485,  0.18819866, -0.5099491 ,\n",
      "        -0.49675047,  0.02660381,  0.38531673,  0.25679654,  0.05693961,\n",
      "         0.4395706 ,  0.19823948, -0.50548947, -0.55762696, -0.64361465]],\n",
      "      dtype=float32), array([[ 1.2618682 , -0.6590534 , -0.58507687, -1.0631719 , -0.16786209,\n",
      "         0.8273939 ,  0.36041695,  0.11267339,  0.3232609 ,  0.8761135 ,\n",
      "        -0.0779699 , -0.5716849 , -0.88505447, -0.29811433, -0.39826953,\n",
      "         0.42376488, -0.0903042 ,  0.12663004,  0.47905803, -1.265808  ,\n",
      "         0.04571982, -1.1762679 , -1.0581298 ,  0.04781076, -0.45249528,\n",
      "        -0.61303025,  0.07201059, -0.9720592 ,  0.0948961 , -1.1508102 ,\n",
      "        -0.16773453, -0.7633733 ,  0.05815914, -0.06652389, -0.13464284,\n",
      "        -0.71617085, -0.95495766,  1.0756586 ,  0.26090088, -0.9781413 ,\n",
      "        -0.9212494 ,  0.04463121,  0.72591645,  0.3996074 ,  0.09771185,\n",
      "         0.726902  ,  0.2948023 , -1.1720939 , -1.0128044 , -1.0639179 ]],\n",
      "      dtype=float32)]\n",
      "Test sentance:  9\n",
      "sentance:  мы умеем переводить\n",
      "origianl translate:  мах тæлмац кæнын зонæм \n",
      "predicted Translate:  дæ ном \n",
      "====================================================================================================\n",
      "\n",
      "[array([[-0.20501931,  0.08945391, -0.21354116, -0.32452804,  0.23126593,\n",
      "        -0.07342282,  0.14834853, -0.18945718, -0.19439393, -0.13745874,\n",
      "         0.18125142,  0.25628373,  0.3233271 , -0.12919982, -0.09059628,\n",
      "        -0.04209391,  0.07655198,  0.15046592, -0.19760182,  0.27712533,\n",
      "        -0.06713693, -0.321382  ,  0.23657073,  0.18185289,  0.20563762,\n",
      "        -0.14571449, -0.2279583 ,  0.31380755, -0.0550871 , -0.2919779 ,\n",
      "         0.06060682, -0.24800278, -0.2962137 ,  0.21671788,  0.2520017 ,\n",
      "         0.25012097,  0.06356964, -0.24334313,  0.10968957, -0.2804821 ,\n",
      "        -0.10534061,  0.21819441, -0.22758992, -0.32636997, -0.1992331 ,\n",
      "        -0.23430088, -0.1152278 , -0.05322774, -0.24635842,  0.13164985]],\n",
      "      dtype=float32), array([[-0.43843317,  0.15420689, -0.52072805, -0.6836312 ,  0.57595986,\n",
      "        -0.18700677,  0.33730537, -0.4102198 , -0.46754652, -0.27357197,\n",
      "         0.4191421 ,  0.6215733 ,  0.65532154, -0.254676  , -0.18096149,\n",
      "        -0.09466305,  0.16568768,  0.3554384 , -0.4062345 ,  0.6198229 ,\n",
      "        -0.1430797 , -0.7832465 ,  0.53831077,  0.4321919 ,  0.47799644,\n",
      "        -0.29230928, -0.48313585,  0.75569916, -0.104758  , -0.61821175,\n",
      "         0.14249828, -0.49957764, -0.7253603 ,  0.49951524,  0.4965037 ,\n",
      "         0.5604761 ,  0.1356563 , -0.4599184 ,  0.24802494, -0.634416  ,\n",
      "        -0.22291368,  0.50172436, -0.42738914, -0.787774  , -0.4514606 ,\n",
      "        -0.51360524, -0.2658559 , -0.09787947, -0.51742655,  0.2947339 ]],\n",
      "      dtype=float32)]\n",
      "Test sentance:  10\n",
      "sentance:  завтра надо идти на работу\n",
      "origianl translate:  райсом мæ куыстмæ цæуын хъæуы \n",
      "predicted Translate:  батырбег равзар \n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    sentance = X_test[i]\n",
    "    original_target = y_test[i]\n",
    "    input_seq = tokenizer_input.texts_to_sequences([sentance])\n",
    "    pad_sequence = pad_sequences(input_seq, maxlen= 30, padding='post')\n",
    "    predicted_target = decode_seq(pad_sequence)\n",
    "    print(\"Test sentance: \",i+1)\n",
    "    print(\"sentance: \",sentance)\n",
    "    print(\"origianl translate:\",original_target[5:-5])\n",
    "    print(\"predicted Translate:\",predicted_target[:-5])\n",
    "    print(\"==\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91079afd-85f2-44cf-98e6-1b57e5734825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
