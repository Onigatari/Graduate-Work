{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3bd3b4-219b-4c3b-a7ee-61d253092206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re, os, time, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle as pkl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import LSTM, GRU, Input, Dense, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83ce1c4c-4d64-4108-8a44-0ee30b289c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d99dca-d5f9-47c8-bb27-2cd56b9b6926",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1337\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2628a3b-41a1-4f5a-ae0b-f0b86b334101",
   "metadata": {},
   "source": [
    "# 1. Подготовка даных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ddd6f5b-f224-40b9-bf48-917cb4828503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w, punctuation=False, toSeq=False):\n",
    "    \"\"\"\n",
    "        Функция для предобработки \n",
    "    \"\"\"\n",
    "    \n",
    "    # Уменьшаем регистр и убираем лишние пробелы\n",
    "    w = w.lower().strip()\n",
    "    \n",
    "    # Замена всех символов 'æ' на однотипный\n",
    "    w = re.sub(r\"ӕ\", r\"æ\", w)\n",
    "    \n",
    "    # Удаление апострофом\n",
    "    w = re.sub(\"'\", '', w)\n",
    "    \n",
    "    if punctuation:\n",
    "        # Делаем между словом и знаком пунктуации отступ 'слово! -> слово !'\n",
    "        w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    else:\n",
    "        # Удаляет все знаки пунктуации\n",
    "        w = re.sub(r\"[^\\w\\s]\", r\"\", w)\n",
    "    \n",
    "    # Выкидываем все остальные символы из рассмотрения \n",
    "    w = re.sub(r\"[^a-яА-Яa-zA-Z?.!,æё]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # Добавляем токены для начала и конца предложения\n",
    "    if toSeq:\n",
    "        w = f'<sos> {w} <eos>'\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06889a21-0434-4753-883e-bf24d25b3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "        Функция, которая создаёт датасет\n",
    "    \"\"\"    \n",
    "    new_path = f'Date/{path}'\n",
    "    with open(new_path, encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    uncleaned_data_list = data.split('\\n')\n",
    "    \n",
    "    source_word = []\n",
    "    target_word = []\n",
    "    for word in uncleaned_data_list:\n",
    "        source_word.append(preprocess_sentence(word.split('\\t')[0], punctuation=False, toSeq=False))\n",
    "        target_word.append(preprocess_sentence(word.split('\\t')[1], punctuation=False, toSeq=True))\n",
    "        \n",
    "    language_data = pd.DataFrame(columns=['Source','Target'])\n",
    "    language_data['Source'] = source_word\n",
    "    language_data['Target'] = target_word\n",
    "    \n",
    "    return language_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a3d01f-6287-43ee-8672-526b327e7dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_(text_data):\n",
    "    '''\n",
    "        Токенайзер\n",
    "    '''\n",
    "    \n",
    "    tokenizer = Tokenizer(filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(text_data)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8401d9-c9c7-402a-a4f5-4b363fe5464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(data):\n",
    "    max_length_ = max([len(x.split(' ')) for x in data])\n",
    "    return max_length_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "298ea3a1-6e0b-4994-b6be-3e235160b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preparing_data(input_seq, output_seq):\n",
    "    tokenizer_input, tokenizer_output = tokenizer_(input_seq), tokenizer_(output_seq)\n",
    "    input_max_length, output_max_length = len(tokenizer_input.word_index) + 1, len(tokenizer_output.word_index) + 1\n",
    "    \n",
    "    return tokenizer_input, tokenizer_output, input_max_length, output_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138a1e2-7510-4933-b3f1-59d31d2efd0b",
   "metadata": {},
   "source": [
    "# 2. Построение модели Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b68f6f-6c03-4d2a-8828-b99c366888d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "        Энкодер \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size_input, HIDDEN_DIM):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.inputs = Input(shape=(None,), name=\"encoder_inputs\")\n",
    "        self.embedding = Embedding(vocab_size_input, HIDDEN_DIM, mask_zero=True, name=\"encoder_embedding\")(self.inputs)\n",
    "        \n",
    "        encoder = LSTM(HIDDEN_DIM, return_state=True, name=\"encoder_lstm\")\n",
    "        self.outputs, state_h, state_c = encoder(self.embedding)\n",
    "        self.states = [state_h, state_c]\n",
    "        \n",
    "def getEncoder(model_loaded):\n",
    "    encoder_inputs_inf = model_loaded.input[0]\n",
    "    encoder_outputs_inf, inf_state_h, inf_state_c = model_loaded.layers[4].output\n",
    "    encoder_inf_states = [inf_state_h,inf_state_c]\n",
    "\n",
    "    return Model(encoder_inputs_inf, encoder_inf_states, name='Encoder') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "454a1f31-5c8f-4274-9b7e-b6a712e855a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "        Декодер \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size_output, HIDDEN_DIM, encoder_states):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.inputs = Input(shape=(None,), name=\"decoder_inputs\")\n",
    "        self.embedding = Embedding(vocab_size_output, HIDDEN_DIM, mask_zero=True, name=\"decoder_embedding\")(self.inputs)\n",
    "        \n",
    "        decoder = LSTM(HIDDEN_DIM, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "        self.outputs, _, _ = decoder(self.embedding, initial_state=encoder_states)\n",
    "        self.dense = Dense(vocab_size_output, activation='softmax', name=\"dense_lstm\")\n",
    "        self.outputs = self.dense(self.outputs)\n",
    "        \n",
    "def getDecoder(model_loaded):\n",
    "    decoder_state_h_input = Input(shape=(HIDDEN_DIM,))\n",
    "    decoder_state_c_input = Input(shape=(HIDDEN_DIM,))\n",
    "    decoder_state_input = [decoder_state_h_input,decoder_state_c_input]\n",
    "\n",
    "    decoder_input_inf = model_loaded.input[1]\n",
    "    decoder_emb_inf = model_loaded.layers[3](decoder_input_inf)\n",
    "    decoder_lstm_inf = model_loaded.layers[5]\n",
    "    decoder_output_inf, decoder_state_h_inf, decoder_state_c_inf = decoder_lstm_inf(decoder_emb_inf, initial_state=decoder_state_input)\n",
    "    decoder_state_inf = [decoder_state_h_inf, decoder_state_c_inf]\n",
    "    dense_inf = model_loaded.layers[6]\n",
    "    decoder_output_final = dense_inf(decoder_output_inf)\n",
    "\n",
    "    return Model([decoder_input_inf]+decoder_state_input, [decoder_output_final]+decoder_state_inf, name='Decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81ad8a6a-e1dc-48de-b446-c2d7ff443ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Batch(X, Y, batch_size):\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_data_input = np.zeros((batch_size,max_lenght_source),dtype='float32') #metrix of batch_size*max_length_english\n",
    "            decoder_data_input = np.zeros((batch_size,max_lenght_target),dtype='float32') #metrix of batch_size*max_length_marathi\n",
    "            decoder_target_input = np.zeros((batch_size,max_lenght_target,vocab_size_target),dtype='float32') # 3d array one hot encoder decoder target data\n",
    "            for i, (input_text,target_text) in enumerate(zip(X[j:j+batch_size],Y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_data_input[i,t] = tokenizer_input.word_index[word] # Here we are storing the encoder \n",
    "                                                                         #seq in row here padding is done automaticaly as \n",
    "                                                                         #we have defined col as max_lenght\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    decoder_data_input[i,t] = tokenizer_output.word_index[word] # same for the decoder sequence\n",
    "                    if t>0:\n",
    "                        decoder_target_input[i,t-1,tokenizer_output.word_index[word]] = 1 #target is one timestep ahead of decoder input because it does not have 'start tag'\n",
    "            yield ([encoder_data_input,decoder_data_input],decoder_target_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a70c8d-84ee-44a6-ba55-6a17657e615d",
   "metadata": {},
   "source": [
    "# 3. Входные параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f10c3abb-6cec-444f-b8fc-dd3b5c769a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "batch_size = 6\n",
    "epochs = 20\n",
    "\n",
    "start_target = \"<sos>\"\n",
    "end_target = \"<eos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e15cd9ae-e669-4474-9ea3-86744e9fb55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'rus-oss.txt'\n",
    "data = load_dataset(path)\n",
    "input_seq, output_seq = data['Source'].values, data['Target'].values\n",
    "tokenizer_input, tokenizer_output, vocab_size_source, vocab_size_target = get_preparing_data(input_seq, output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d268c4ac-2e11-40ab-bb2a-7e59897bc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path[:-4]}-tokenizer_input.pkl','wb') as f:\n",
    "    pkl.dump(tokenizer_input, f)\n",
    "\n",
    "with open(f'{path[:-4]}-tokenizer_output.pkl','wb') as f:\n",
    "    pkl.dump(tokenizer_output, f)\n",
    "\n",
    "pkl.dump(tokenizer_input, open(f'{path[:-4]}-tokenizer_input.pkl', 'wb'))\n",
    "pkl.dump(tokenizer_output, open(f'{path[:-4]}-tokenizer_output.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc0848b-e173-4914-84c0-aef68b3845cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_seq, output_seq, test_size = 0.1)\n",
    "train_samples = len(X_train)\n",
    "test_samples = len(X_test)\n",
    "\n",
    "max_lenght_source = max_length(X_train)\n",
    "max_lenght_target = max_length(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8fb90c3-e972-4fa3-94ca-0b3c1067b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size_source, HIDDEN_DIM)\n",
    "decoder = Decoder(vocab_size_target, HIDDEN_DIM, encoder.states)\n",
    "\n",
    "model = Model([encoder.inputs, decoder.inputs], decoder.outputs, name=\"LSTM-Translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c28ea3a-7a69-4e5b-a24a-a6c4145f6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = [\n",
    "    'categorical_crossentropy',\n",
    "    'binary_crossentropy',\n",
    "]\n",
    "\n",
    "now_loss_function = loss_function[0]\n",
    "model.compile(loss=now_loss_function, optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6409fe07-d9e8-440f-a5bb-ce9ed3d5d4d7",
   "metadata": {},
   "source": [
    "# 4. Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2760af39-9923-4671-8de5-012a2db00f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ONIGAT~1\\AppData\\Local\\Temp/ipykernel_7040/1321299230.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator = Batch(X_train, y_train, batch_size = batch_size), steps_per_epoch = train_samples//batch_size, epochs=epochs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 10s 54ms/step - loss: 1.2815 - accuracy: 0.1566\n",
      "Epoch 2/20\n",
      "70/70 [==============================] - 4s 57ms/step - loss: 1.1754 - accuracy: 0.1690\n",
      "Epoch 3/20\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 1.1216 - accuracy: 0.1711\n",
      "Epoch 4/20\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 1.0644 - accuracy: 0.1735\n",
      "Epoch 5/20\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 1.0283 - accuracy: 0.1767\n",
      "Epoch 6/20\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 0.9812 - accuracy: 0.1790\n",
      "Epoch 7/20\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 0.9324 - accuracy: 0.1868\n",
      "Epoch 8/20\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 0.8906 - accuracy: 0.1952\n",
      "Epoch 9/20\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 0.8513 - accuracy: 0.2051\n",
      "Epoch 10/20\n",
      "70/70 [==============================] - 4s 56ms/step - loss: 0.8114 - accuracy: 0.2210\n",
      "Epoch 11/20\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 0.7753 - accuracy: 0.2286\n",
      "Epoch 12/20\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 0.7335 - accuracy: 0.2486\n",
      "Epoch 13/20\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 0.7027 - accuracy: 0.2593\n",
      "Epoch 14/20\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 0.6686 - accuracy: 0.2734\n",
      "Epoch 15/20\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 0.6346 - accuracy: 0.3002\n",
      "Epoch 16/20\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 0.6032 - accuracy: 0.3229\n",
      "Epoch 17/20\n",
      "70/70 [==============================] - 4s 54ms/step - loss: 0.5640 - accuracy: 0.3619\n",
      "Epoch 18/20\n",
      "70/70 [==============================] - 4s 55ms/step - loss: 0.5308 - accuracy: 0.3903\n",
      "Epoch 19/20\n",
      "70/70 [==============================] - 4s 53ms/step - loss: 0.5005 - accuracy: 0.4242\n",
      "Epoch 20/20\n",
      "70/70 [==============================] - 4s 51ms/step - loss: 0.4690 - accuracy: 0.4537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2283ff32040>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%capture\n",
    "model.fit_generator(generator = Batch(X_train, y_train, batch_size = batch_size), steps_per_epoch = train_samples//batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f6a63-4868-4380-a9ec-3ea3069b8192",
   "metadata": {},
   "source": [
    "# 5. Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ebbda12-686d-47de-af07-63661bd10b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отрисовка схемы модели\n",
    "# plot_model(model, to_file=f'{dir}-{epochs}-train_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7db8682d-539a-4ace-9c95-3f414ee4fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save_JSON():\n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    with open(f'LSTM-[{path[:-4]}]-[Epochs={epochs}]-[LossFunction={now_loss_function}].json', \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    model.save_weights(f'LSTM-[{path[:-4]}]-[Epochs={epochs}]-[LossFunction={now_loss_function}]-[weight].h5')\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a270dfdd-590f-46f9-88ac-331d15f8b34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_save_JSON()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa48a794-6ae1-4ac4-bf9a-c57c1fa1918e",
   "metadata": {},
   "source": [
    "# 6. Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7155f866-a36c-4996-a56b-ee5682ae6e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load_JSON():\n",
    "    json_file = open(f'LSTM-[{path[:-4]}]-[Epochs={epochs}]-[LossFunction={now_loss_function}].json')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_loaded = model_from_json(loaded_model_json)\n",
    "\n",
    "    model_loaded.load_weights(f'LSTM-[{path[:-4]}]-[Epochs={epochs}]-[LossFunction={now_loss_function}]-[weight].h5')\n",
    "    print(\"Model loaded\")\n",
    "    \n",
    "    return model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd55a3e8-a44f-4df8-b659-eb8d10788911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model_loaded = model_load_JSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dbc917e-9bdb-4b2d-993d-209fc0eb5a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM-Translation\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " encoder_embedding (Embedding)  (None, None, 256)    241920      ['encoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " decoder_embedding (Embedding)  (None, None, 256)    236032      ['decoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)            [(None, 256),        525312      ['encoder_embedding[0][0]']      \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 256),  525312      ['decoder_embedding[0][0]',      \n",
      "                                 (None, 256),                     'encoder_lstm[0][1]',           \n",
      "                                 (None, 256)]                     'encoder_lstm[0][2]']           \n",
      "                                                                                                  \n",
      " dense_lstm (Dense)             (None, None, 922)    236954      ['decoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,765,530\n",
      "Trainable params: 1,765,530\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba7c15b6-9095-453e-abb7-465a7504273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = getEncoder(model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28f3ab23-a88c-47ab-8791-53785ae80f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = getDecoder(model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d118094-3de5-4929-8071-403404efc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path[:-4]}-tokenizer_input.pkl','rb') as f:\n",
    "    tokenizer_input = pkl.load(f)\n",
    "with open(f'{path[:-4]}-tokenizer_output.pkl','rb') as f:\n",
    "    tokenizer_output = pkl.load(f)\n",
    "\n",
    "reverse_word_map_input = dict(map(reversed, tokenizer_input.word_index.items()))\n",
    "reverse_word_map_target = dict(map(reversed, tokenizer_output.word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530156f-4eb4-4639-9188-6e3a5f2868e5",
   "metadata": {},
   "source": [
    "# 7. Проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2c2f350-557d-4264-bc12-0f1d28d0f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(input_seq):\n",
    "    state_values_encoder = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tokenizer_output.word_index[start_target]\n",
    "    stop_condition = False\n",
    "    decoder_sentance = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        sample_word , decoder_h,decoder_c= decoder_model.predict([target_seq] + state_values_encoder)\n",
    "        sample_word_index = np.argmax(sample_word[0,-1,:])\n",
    "        decoder_word = reverse_word_map_target[sample_word_index]\n",
    "        decoder_sentance += ' '+ decoder_word\n",
    "        if (decoder_word == end_target or \n",
    "            len(decoder_sentance) > 70):\n",
    "            stop_condition = True\n",
    "        target_seq[0, 0] = sample_word_index\n",
    "        state_values_encoder = [decoder_h,decoder_c]\n",
    "    return decoder_sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99ee0b5e-9bc3-4102-851f-3b0f80afc344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:\t 1\n",
      "  Sentance:\t здесь не бываю\n",
      "  Origianl:\t  ам нæ вæййын \n",
      "  Predicted:\t  нæ дæн \n",
      "  BLEU:\t 0.3333333333333333\n",
      "====================================================================================================\n",
      "Test:\t 2\n",
      "  Sentance:\t хорошо\n",
      "  Origianl:\t  хорз \n",
      "  Predicted:\t  хорз \n",
      "  BLEU:\t 1.0\n",
      "====================================================================================================\n",
      "Test:\t 3\n",
      "  Sentance:\t ктото позвонил\n",
      "  Origianl:\t  чидæр æрбадзырдта \n",
      "  Predicted:\t  камерæ айс \n",
      "  BLEU:\t 0\n",
      "====================================================================================================\n",
      "Test:\t 4\n",
      "  Sentance:\t кто в этом году идет в школу\n",
      "  Origianl:\t  ацы аз скъоламæ чи цæудзæн \n",
      "  Predicted:\t  ацы аз скъоламæ скъоламæ \n",
      "  BLEU:\t 0.6\n",
      "====================================================================================================\n",
      "Test:\t 5\n",
      "  Sentance:\t где ты отдыхал\n",
      "  Origianl:\t  дæ фæллад кæм уагътай \n",
      "  Predicted:\t  кæм дæ хорзæхæй \n",
      "  BLEU:\t 0.5\n",
      "====================================================================================================\n",
      "Test:\t 6\n",
      "  Sentance:\t кот\n",
      "  Origianl:\t  гæды \n",
      "  Predicted:\t  фос \n",
      "  BLEU:\t 0\n",
      "====================================================================================================\n",
      "Test:\t 7\n",
      "  Sentance:\t у меня нет денег но есть мечты\n",
      "  Origianl:\t  æхца мæм нæй фæлæ мæм ис бæллицтæ \n",
      "  Predicted:\t  уый дæр дæр дæр дæр ис ахуырадон кълас \n",
      "  BLEU:\t 0.12383969996431167\n",
      "====================================================================================================\n",
      "Test:\t 8\n",
      "  Sentance:\t счастье вашему дому\n",
      "  Origianl:\t  фарн уæ хæдзары \n",
      "  Predicted:\t  алфавиты дамгъæ равзар \n",
      "  BLEU:\t 0\n",
      "====================================================================================================\n",
      "Test:\t 9\n",
      "  Sentance:\t жадный\n",
      "  Origianl:\t  кæрæф \n",
      "  Predicted:\t  фос \n",
      "  BLEU:\t 0\n",
      "====================================================================================================\n",
      "Test:\t 10\n",
      "  Sentance:\t том и мэри танцевали вместе\n",
      "  Origianl:\t  том æмæ мери иумæ кафыдысты \n",
      "  Predicted:\t  том бон у том ферох и \n",
      "  BLEU:\t 0.1637461506155964\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    sentance = X_train[i]\n",
    "    original_target = y_train[i]\n",
    "    input_seq = tokenizer_input.texts_to_sequences([sentance])\n",
    "    pad_sequence = pad_sequences(input_seq, maxlen= 30, padding='post')\n",
    "    predicted_target = decode_seq(pad_sequence)\n",
    "    reference = original_target[5:-5].strip().split()\n",
    "    hypothetic = predicted_target[:-5].strip().split()\n",
    "    \n",
    "    print(\"Test:\\t\", i+1)\n",
    "    print(\"  Sentance:\\t\", sentance)\n",
    "    print(\"  Origianl:\\t\", original_target[5:-5])\n",
    "    print(\"  Predicted:\\t\", predicted_target[:-5])\n",
    "    print(\"  BLEU:\\t\", sentence_bleu([hypothetic], reference, weights = [1]))\n",
    "    print(\"==\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef51dc10-c109-429e-9bfa-34ca38f348f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:\t 1\n",
      "  Sentance:\t нет я работал у меня отпуск будет зимой\n",
      "  Origianl:\t  нæ кусгæ кæнын мæнæн ме нцойуагъд зымæджы уыдзæн \n",
      "  Predicted:\t  æз дæр дæр дæр дæр нæ федтай \n",
      "  BLEU:\t 0.12500000000000003\n",
      "====================================================================================================\n",
      "Test:\t 2\n",
      "  Sentance:\t расскажите нам всё что знаете\n",
      "  Origianl:\t  радзурут нын алцыдæр цы зонут \n",
      "  Predicted:\t  нæ цы æрцыди дыл нæу \n",
      "  BLEU:\t 0.2\n",
      "====================================================================================================\n",
      "Test:\t 3\n",
      "  Sentance:\t у нас возле дома есть автобусная остановка\n",
      "  Origianl:\t  нæ хæдзары цур ис автобусы æрлæууæн \n",
      "  Predicted:\t  мæ фыд дыууæ нана уайтагъд ног \n",
      "  BLEU:\t 0\n",
      "====================================================================================================\n",
      "Test:\t 4\n",
      "  Sentance:\t он его нашёл\n",
      "  Origianl:\t  уый йæ ссардта \n",
      "  Predicted:\t  уый бон у \n",
      "  BLEU:\t 0.3333333333333333\n",
      "====================================================================================================\n",
      "Test:\t 5\n",
      "  Sentance:\t мои уши красные\n",
      "  Origianl:\t  мæ хъустæ сырx сты \n",
      "  Predicted:\t  дæ ном куыд алан \n",
      "  BLEU:\t 0\n",
      "====================================================================================================\n",
      "Test:\t 6\n",
      "  Sentance:\t том ест корм для кошек\n",
      "  Origianl:\t  том хæры гæдиты хæринаг \n",
      "  Predicted:\t  том мæнæй том дыл и \n",
      "  BLEU:\t 0.19470019576785122\n",
      "====================================================================================================\n",
      "Test:\t 7\n",
      "  Sentance:\t все хорошо спасибо\n",
      "  Origianl:\t  бузныг ницы мын у \n",
      "  Predicted:\t  хорз хорз дæн \n",
      "  BLEU:\t 0\n",
      "====================================================================================================\n",
      "Test:\t 8\n",
      "  Sentance:\t кушать\n",
      "  Origianl:\t  хæрын \n",
      "  Predicted:\t  фос \n",
      "  BLEU:\t 0\n",
      "====================================================================================================\n",
      "Test:\t 9\n",
      "  Sentance:\t мы умеем переводить\n",
      "  Origianl:\t  мах тæлмац кæнын зонæм \n",
      "  Predicted:\t  дæ мыггаг кæмæй \n",
      "  BLEU:\t 0\n",
      "====================================================================================================\n",
      "Test:\t 10\n",
      "  Sentance:\t завтра надо идти на работу\n",
      "  Origianl:\t  райсом мæ куыстмæ цæуын хъæуы \n",
      "  Predicted:\t  кæй цы кодтай нæу \n",
      "  BLEU:\t 0\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    sentance = X_test[i]\n",
    "    original_target = y_test[i]\n",
    "    input_seq = tokenizer_input.texts_to_sequences([sentance])\n",
    "    pad_sequence = pad_sequences(input_seq, maxlen= 30, padding='post')\n",
    "    predicted_target = decode_seq(pad_sequence)\n",
    "    reference = original_target[5:-5].strip().split()\n",
    "    hypothetic = predicted_target[:-5].strip().split()\n",
    "    \n",
    "    print(\"Test:\\t\", i+1)\n",
    "    print(\"  Sentance:\\t\", sentance)\n",
    "    print(\"  Origianl:\\t\", original_target[5:-5])\n",
    "    print(\"  Predicted:\\t\", predicted_target[:-5])\n",
    "    print(\"  BLEU:\\t\", sentence_bleu([hypothetic], reference, weights = [1]))\n",
    "    print(\"==\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91079afd-85f2-44cf-98e6-1b57e5734825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
