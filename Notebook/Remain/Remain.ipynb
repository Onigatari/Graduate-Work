{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3bd3b4-219b-4c3b-a7ee-61d253092206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Input, Dense,Embedding\n",
    "from keras.models import Model,load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4dd2f02-e395-49ef-83a5-561264659141",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'eng-mar.txt'\n",
    "cur_path = os.path.abspath('')\n",
    "new_path = os.path.relpath(f'../Date/{dir}', cur_path)\n",
    "with open(new_path, encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8594834f-8c22-42f2-9c9d-824682476705",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data_list = data.split('\\n')\n",
    "uncleaned_data_list = uncleaned_data_list[:38695]\n",
    "\n",
    "rus_word = []\n",
    "oss_word = []\n",
    "\n",
    "for word in uncleaned_data_list:\n",
    "    rus_word.append(word.split('\\t')[0])\n",
    "    oss_word.append(word.split('\\t')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f26833-35c5-4c74-be15-a6aeee953077",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_data = pd.DataFrame(columns=['Russia','Ossetian'])\n",
    "language_data['Russia'] = rus_word\n",
    "language_data['Ossetian'] = oss_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b1b8388-5baf-46d9-82e5-f82e11c68096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to csv\n",
    "language_data.to_csv(f'{dir}-language_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61839196-3044-4a75-9c9d-d31d92c56e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data from csv\n",
    "language_data = pd.read_csv(f'{dir}-language_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74166bf2-1b9e-4c76-97cb-dcc10a6a832d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Russia</th>\n",
       "      <th>Ossetian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>जा.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>पळ!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>धाव!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>पळा!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>धावा!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Russia Ossetian\n",
       "0    Go.      जा.\n",
       "1   Run!      पळ!\n",
       "2   Run!     धाव!\n",
       "3   Run!     पळा!\n",
       "4   Run!    धावा!"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfaae43d-4b63-4b8e-9cfd-374bed9eaea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Russia</th>\n",
       "      <th>Ossetian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38690</th>\n",
       "      <td>If religion were synonymous with morality, Bra...</td>\n",
       "      <td>जर धर्म व नीतिमत्ता समानार्थी शब्द असते, तर ब्...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38691</th>\n",
       "      <td>Just saying you don't like fish because of the...</td>\n",
       "      <td>हड्डींमुळे मासे आवडत नाही असं म्हणणं हे काय मा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38692</th>\n",
       "      <td>The Japanese Parliament today officially elect...</td>\n",
       "      <td>आज जपानी संसदेने अधिकृतरित्या र्‍यौतारौ हाशिमो...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38693</th>\n",
       "      <td>Tom tried to sell his old VCR instead of throw...</td>\n",
       "      <td>टॉमने त्याचा जुना व्ही.सी.आर फेकून टाकण्याऐवजी...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38694</th>\n",
       "      <td>You can't view Flash content on an iPad. Howev...</td>\n",
       "      <td>आयपॅडवर फ्लॅश आशय बघता येत नाही. पण तुम्ही त्य...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Russia  \\\n",
       "38690  If religion were synonymous with morality, Bra...   \n",
       "38691  Just saying you don't like fish because of the...   \n",
       "38692  The Japanese Parliament today officially elect...   \n",
       "38693  Tom tried to sell his old VCR instead of throw...   \n",
       "38694  You can't view Flash content on an iPad. Howev...   \n",
       "\n",
       "                                                Ossetian  \n",
       "38690  जर धर्म व नीतिमत्ता समानार्थी शब्द असते, तर ब्...  \n",
       "38691  हड्डींमुळे मासे आवडत नाही असं म्हणणं हे काय मा...  \n",
       "38692  आज जपानी संसदेने अधिकृतरित्या र्‍यौतारौ हाशिमो...  \n",
       "38693  टॉमने त्याचा जुना व्ही.सी.आर फेकून टाकण्याऐवजी...  \n",
       "38694  आयपॅडवर फ्लॅश आशय बघता येत नाही. पण तुम्ही त्य...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d07c12e3-2ae9-4ff9-910d-157045ae2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_text = language_data['Russia'].values\n",
    "oss_text = language_data['Ossetian'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d4e6e5-6446-49c1-b5b3-2f06c0caf170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Go.', 'जा.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus_text[0], oss_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9becfba-23ea-4161-baf7-db337fff0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercasing the setences\n",
    "rus_text_ = [x.lower() for x in rus_text]\n",
    "oss_text_ = [x.lower() for x in oss_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a263f311-c1f2-4398-9cb7-25b657d87f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_text_ = [re.sub(\"'\",'',x) for x in rus_text_]\n",
    "oss_text_ = [re.sub(\"'\",'',x) for x in oss_text_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47301419-81a6-4873-b20f-238a0207d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove puntuation\n",
    "def remove_punc(text_list):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    removed_punc_text = []\n",
    "    for sent in text_list:\n",
    "        sentance = [w.translate(table) for w in sent.split(' ')]\n",
    "        removed_punc_text.append(' '.join(sentance))\n",
    "    return removed_punc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a57960b-aab3-4e15-b1d2-f5409561525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_text_ = remove_punc(rus_text_)\n",
    "oss_text_ = remove_punc(oss_text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5251ea54-15e2-48db-a9b8-95485cfd8ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the digits from russian sentances\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "removed_digits_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a1752b7-0ec5-4ed7-81ae-da8ae2f9cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in rus_text_:\n",
    "    sentance = [w.translate(remove_digits) for w in sent.split(' ')]\n",
    "    removed_digits_text.append(' '.join(sentance))\n",
    "    \n",
    "rus_text_ = removed_digits_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "430f9bf7-974c-4c5c-9f08-20f196c87fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_text_ = [x.strip() for x in rus_text_]\n",
    "oss_text_ = [x.strip() for x in oss_text_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "969fe0cb-c166-451b-8c9f-db502220db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_text_ = [re.sub(r\"([,.!?])\", r\"\", x) for x in rus_text_]\n",
    "oss_text_ = [re.sub(r\"([,.!?])\", r\"\", x) for x in oss_text_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b11ab261-579e-4796-9270-b7412d744761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rus_text_ = [\"<sos> \" + x + \" <eos>\" for x in rus_text_]\n",
    "oss_text_ = [\"start \" + x + \" end\" for x in oss_text_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31e666cc-a2c4-44fa-b2be-5fce7a707e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('start जा end', 'go')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oss_text_[0], rus_text_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66aacda-d7c5-432e-b4ef-26894828d82b",
   "metadata": {},
   "source": [
    "# Data spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cd1a947-3809-44e3-9a95-f7beb88e9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rus_text_\n",
    "Y = oss_text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f162257-a1b4-4f95-8924-c0a8d8cfec23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34825, 34825, 3870, 3870)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.1)\n",
    "len(X_train),len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "831ccdc4-2ac5-47a6-bdf8-03d08ecd2652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('go', 'start जा end')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], Y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a866d-44e2-47a3-8347-8f343cadafe6",
   "metadata": {},
   "source": [
    "## Data preparing for encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "979cfe58-63c5-4c4a-be81-23caf194cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for the word embedding\n",
    "def Max_length(data):\n",
    "    max_length_ = max([len(x.split(' ')) for x in data])\n",
    "    return max_length_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8a27121-00cd-48b0-93bb-d83113a79c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data\n",
    "max_length_rus = Max_length(X_train)\n",
    "max_lenght_oss = Max_length(y_train)\n",
    "\n",
    "#Test data\n",
    "max_length_rus_test = Max_length(X_test)\n",
    "max_lenght_oss_test = Max_length(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4af652ed-5759-4cce-8c21-1c54464057d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 26)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_rus, max_lenght_oss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "830d97c3-5032-4a1d-9cc8-4a5e113bfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_(text_data):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text_data)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer_input = tokenizer_(X_train)\n",
    "vocab_size_input = len(tokenizer_input.word_index) + 1\n",
    "tokenizer_target = tokenizer_(y_train)\n",
    "vocab_size_target = len(tokenizer_target.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9c2df04-8720-4c4f-b2d9-8900f4bd7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{dir}-tokenizer_input.pkl','wb') as f:\n",
    "    pkl.dump(tokenizer_input,f)\n",
    "\n",
    "with open(f'{dir}-tokenizer_target.pkl','wb') as f:\n",
    "    pkl.dump(tokenizer_target,f)\n",
    "    \n",
    "pkl.dump(tokenizer_input, open(f'{dir}-tokenizer_input.pkl', 'wb'))\n",
    "pkl.dump(tokenizer_target, open(f'{dir}-tokenizer_target.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "403c63c9-7a66-403b-a79f-9e06fcee84e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5412, 12907)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_input, vocab_size_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81ad8a6a-e1dc-48de-b446-c2d7ff443ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_batch(X= X_train,Y=y_train, batch_size=128):\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_data_input = np.zeros((batch_size,max_length_rus),dtype='float32') #metrix of batch_size*max_length_english\n",
    "            decoder_data_input = np.zeros((batch_size,max_lenght_oss),dtype='float32') #metrix of batch_size*max_length_marathi\n",
    "            decoder_target_input = np.zeros((batch_size,max_lenght_oss,vocab_size_target),dtype='float32') # 3d array one hot encoder decoder target data\n",
    "            for i, (input_text,target_text) in enumerate(zip(X[j:j+batch_size],Y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_data_input[i,t] = tokenizer_input.word_index[word] # Here we are storing the encoder \n",
    "                                                                         #seq in row here padding is done automaticaly as \n",
    "                                                                         #we have defined col as max_lenght\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    # if word == 'START_':\n",
    "                    #   word = 'start'\n",
    "                    # elif word == 'END_':\n",
    "                    #   word = 'end'\n",
    "                    decoder_data_input[i,t] = tokenizer_target.word_index[word] # same for the decoder sequence\n",
    "                    if t>0:\n",
    "                        decoder_target_input[i,t-1,tokenizer_target.word_index[word]] = 1 #target is one timestep ahead of decoder input because it does not have 'start tag'\n",
    "            # print(encoder_data_input.shape())\n",
    "            yield ([encoder_data_input,decoder_data_input],decoder_target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cecaf5a-26f8-4b12-a372-d88732504b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,),name=\"encoder_inputs\")\n",
    "emb_layer_encoder = Embedding(vocab_size_input,latent_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(emb_layer_encoder)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,),name=\"decoder_inputs\")\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "emb_layer_decoder = Embedding(vocab_size_target,latent_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(emb_layer_decoder, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_target, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63153d0e-941a-4bf0-87eb-ae29652c09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ebbda12-686d-47de-af07-63661bd10b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, to_file=f'{dir}-train_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d12c688a-25f0-426a-b9dd-873ee155dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec1d51b8-bb57-4260-a64f-1e08364e1a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ONIGAT~1\\AppData\\Local\\Temp/ipykernel_2816/1782680856.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator = generator_batch(X_train, y_train, batch_size = batch_size), steps_per_epoch = train_samples//batch_size, epochs=epochs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "272/272 [==============================] - 228s 783ms/step - loss: 1.4128 - accuracy: 0.1496\n",
      "Epoch 2/50\n",
      "272/272 [==============================] - 218s 800ms/step - loss: 1.2276 - accuracy: 0.1745\n",
      "Epoch 3/50\n",
      "272/272 [==============================] - 232s 852ms/step - loss: 1.1731 - accuracy: 0.1896\n",
      "Epoch 4/50\n",
      "272/272 [==============================] - 236s 866ms/step - loss: 1.1322 - accuracy: 0.2020\n",
      "Epoch 5/50\n",
      "272/272 [==============================] - 231s 850ms/step - loss: 1.0962 - accuracy: 0.2158\n",
      "Epoch 6/50\n",
      "272/272 [==============================] - 240s 884ms/step - loss: 1.0584 - accuracy: 0.2314\n",
      "Epoch 7/50\n",
      "272/272 [==============================] - 217s 799ms/step - loss: 1.0245 - accuracy: 0.2439\n",
      "Epoch 8/50\n",
      "272/272 [==============================] - 217s 798ms/step - loss: 0.9904 - accuracy: 0.2561\n",
      "Epoch 9/50\n",
      "272/272 [==============================] - 182s 669ms/step - loss: 0.9590 - accuracy: 0.2692\n",
      "Epoch 10/50\n",
      "272/272 [==============================] - 180s 662ms/step - loss: 0.9304 - accuracy: 0.2819\n",
      "Epoch 11/50\n",
      "272/272 [==============================] - 186s 683ms/step - loss: 0.9031 - accuracy: 0.2941\n",
      "Epoch 12/50\n",
      "272/272 [==============================] - 182s 668ms/step - loss: 0.8768 - accuracy: 0.3054\n",
      "Epoch 13/50\n",
      "272/272 [==============================] - 181s 666ms/step - loss: 0.8520 - accuracy: 0.3167\n",
      "Epoch 14/50\n",
      "272/272 [==============================] - 181s 664ms/step - loss: 0.8291 - accuracy: 0.3270\n",
      "Epoch 15/50\n",
      "272/272 [==============================] - 203s 745ms/step - loss: 0.8071 - accuracy: 0.3369\n",
      "Epoch 16/50\n",
      "272/272 [==============================] - 215s 790ms/step - loss: 0.7864 - accuracy: 0.3467\n",
      "Epoch 17/50\n",
      "272/272 [==============================] - 219s 805ms/step - loss: 0.7667 - accuracy: 0.3560\n",
      "Epoch 18/50\n",
      "  4/272 [..............................] - ETA: 3:54 - loss: 0.7306 - accuracy: 0.3663"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ONIGAT~1\\AppData\\Local\\Temp/ipykernel_2816/1782680856.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# %%capture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_samples\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2207\u001b[0m         \u001b[1;34m'Please use `Model.fit`, which supports generators.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2208\u001b[0m         stacklevel=2)\n\u001b[1;32m-> 2209\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   2210\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2211\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "model.fit_generator(generator = generator_batch(X_train, y_train, batch_size = batch_size), steps_per_epoch = train_samples//batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db8682d-539a-4ace-9c95-3f414ee4fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(f'{dir}-model_2.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(f'{dir}-model_weight_5.h5')\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e412cb7-8fb6-4910-85e2-8c4ea3f10c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model architecture and asigning the weights\n",
    "json_file = open(f'{dir}-model_2.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_loaded = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_loaded.load_weights(f'{dir}-model_weight_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54628c36-6922-4c50-af79-715640f4bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50\n",
    "#inference encoder\n",
    "encoder_inputs_inf = model_loaded.input[0] #Trained encoder input layer\n",
    "encoder_outputs_inf, inf_state_h, inf_state_c = model_loaded.layers[4].output # retoring the encoder lstm output and states\n",
    "encoder_inf_states = [inf_state_h,inf_state_c]\n",
    "encoder_model = Model(encoder_inputs_inf,encoder_inf_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac9691-08ef-462c-ba5a-255d2d9bb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50\n",
    "#inference encoder\n",
    "encoder_inputs_inf = model_loaded.input[0] #Trained encoder input layer\n",
    "encoder_outputs_inf, inf_state_h, inf_state_c = model_loaded.layers[4].output # retoring the encoder lstm output and states\n",
    "encoder_inf_states = [inf_state_h,inf_state_c]\n",
    "encoder_model = Model(encoder_inputs_inf,encoder_inf_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d5383-b268-427f-abca-67bcdbd59614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference decoder\n",
    "# The following tensor will store the state of the previous timestep in the \"starting the encoder final time step\"\n",
    "decoder_state_h_input = Input(shape=(latent_dim,)) #becase during training we have set the lstm unit to be of 50\n",
    "decoder_state_c_input = Input(shape=(latent_dim,))\n",
    "decoder_state_input = [decoder_state_h_input,decoder_state_c_input]\n",
    "\n",
    "# # inference decoder input\n",
    "decoder_input_inf = model_loaded.input[1] #Trained decoder input layer\n",
    "# decoder_input_inf._name='decoder_input'\n",
    "decoder_emb_inf = model_loaded.layers[3](decoder_input_inf)\n",
    "decoder_lstm_inf = model_loaded.layers[5]\n",
    "decoder_output_inf, decoder_state_h_inf, decoder_state_c_inf = decoder_lstm_inf(decoder_emb_inf, initial_state =decoder_state_input)\n",
    "decoder_state_inf = [decoder_state_h_inf,decoder_state_c_inf]\n",
    "#inference dense layer\n",
    "dense_inf = model_loaded.layers[6]\n",
    "decoder_output_final = dense_inf(decoder_output_inf)# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "decoder_model = Model([decoder_input_inf]+decoder_state_input,[decoder_output_final]+decoder_state_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5c70c-a9ef-4da6-a1d5-fe34fe94238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{dir}-tokenizer_input.pkl','rb') as f:\n",
    "    tokenizer_input = pkl.load(f)\n",
    "with open(f'{dir}-tokenizer_target.pkl','rb') as f:\n",
    "    tokenizer_target = pkl.load(f)\n",
    "# Creating the reverse mapping to get the word from the index in the sequence\n",
    "reverse_word_map_input = dict(map(reversed, tokenizer_input.word_index.items()))\n",
    "reverse_word_map_target = dict(map(reversed, tokenizer_target.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714b9ba-6a9e-4b55-a8af-60f8ca3da9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(input_seq):\n",
    "    # print(\"input_seq=>\",input_seq)\n",
    "    state_values_encoder = encoder_model.predict(input_seq)\n",
    "    # intialize the target seq with start tag\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tokenizer_target.word_index['start']\n",
    "    # print(\"target_seq:=>\",target_seq)\n",
    "    stop_condition = False\n",
    "    decoder_sentance = ''\n",
    "    # print(\"Beforee the while loop\")\n",
    "    while not stop_condition:\n",
    "        sample_word , decoder_h,decoder_c= decoder_model.predict([target_seq] + state_values_encoder)\n",
    "        # print(\"sample_word: =>\",sample_word)\n",
    "        sample_word_index = np.argmax(sample_word[0,-1,:])\n",
    "        # print(\"sample_word_index: \",sample_word_index)\n",
    "        decoder_word = reverse_word_map_target[sample_word_index]\n",
    "        decoder_sentance += ' '+ decoder_word\n",
    "        # print(\"decoded word:=>\",decoder_word)\n",
    "        # print(len(decoder_sentance))\n",
    "        # print(\"len(decoder_sentance) > 70: \",len(decoder_sentance) > 70)\n",
    "        # print('decoder_word == \"end\"',decoder_word == 'end')\n",
    "        # print(decoder_word == 'end' or len(decoder_sentance) > 70)\n",
    "        # stop condition for the while loop\n",
    "        if (decoder_word == 'end' or \n",
    "            len(decoder_sentance) > 70):\n",
    "            stop_condition = True\n",
    "            # print(\"from if condition\")\n",
    "        # target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sample_word_index\n",
    "        # print(target_seq)\n",
    "        state_values_encoder = [decoder_h,decoder_c]\n",
    "    return decoder_sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03d272-1854-4bb8-8c55-d48a42a2e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    sentance = X_test[i]\n",
    "    original_target = y_test[i]\n",
    "    input_seq = tokenizer_input.texts_to_sequences([sentance])\n",
    "    pad_sequence = pad_sequences(input_seq, maxlen= 30, padding='post')\n",
    "    # print('input_sequence =>',input_seq)\n",
    "    # print(\"pad_seq=>\",pad_sequence)\n",
    "    predicted_target = decode_seq(pad_sequence)\n",
    "    print(\"Test sentance: \",i+1)\n",
    "    print(\"sentance: \",sentance)\n",
    "    print(\"origianl translate:\",original_target[6:-4])\n",
    "    print(\"predicted Translate:\",predicted_target[:-4])\n",
    "    print(\"==\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d8d26-c01c-469f-9b42-9c94dcaf4670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
