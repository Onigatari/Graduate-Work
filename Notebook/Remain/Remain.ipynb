{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3bd3b4-219b-4c3b-a7ee-61d253092206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Input, Dense,Embedding\n",
    "from keras.models import Model,load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4dd2f02-e395-49ef-83a5-561264659141",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'rus-eng.txt'\n",
    "cur_path = os.path.abspath('')\n",
    "new_path = os.path.relpath(f'../Date/{dir}', cur_path)\n",
    "with open(new_path, encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8594834f-8c22-42f2-9c9d-824682476705",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data_list = data.split('\\n')\n",
    "uncleaned_data_list = uncleaned_data_list[:38695]\n",
    "\n",
    "source_word = []\n",
    "target_word = []\n",
    "start_target = \"sos\"\n",
    "end_target = \"eos\"\n",
    "\n",
    "for word in uncleaned_data_list:\n",
    "    source_word.append(word.split('\\t')[0])\n",
    "    target_word.append(word.split('\\t')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f26833-35c5-4c74-be15-a6aeee953077",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_data = pd.DataFrame(columns=['Source','Target'])\n",
    "language_data['Source'] = source_word\n",
    "language_data['Target'] = target_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b1b8388-5baf-46d9-82e5-f82e11c68096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to csv\n",
    "language_data.to_csv(f'{dir}-language_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61839196-3044-4a75-9c9d-d31d92c56e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data from csv\n",
    "language_data = pd.read_csv(f'{dir}-language_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74166bf2-1b9e-4c76-97cb-dcc10a6a832d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Марш!</td>\n",
       "      <td>Go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Иди.</td>\n",
       "      <td>Go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Идите.</td>\n",
       "      <td>Go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Здравствуйте.</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Привет!</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Source Target\n",
       "0          Марш!    Go.\n",
       "1           Иди.    Go.\n",
       "2         Идите.    Go.\n",
       "3  Здравствуйте.    Hi.\n",
       "4        Привет!    Hi."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfaae43d-4b63-4b8e-9cfd-374bed9eaea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38690</th>\n",
       "      <td>Ты не сумасшедшая.</td>\n",
       "      <td>You're not crazy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38691</th>\n",
       "      <td>Вы не сумасшедший.</td>\n",
       "      <td>You're not crazy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38692</th>\n",
       "      <td>Вы не сумасшедшая.</td>\n",
       "      <td>You're not crazy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38693</th>\n",
       "      <td>Ты не умираешь.</td>\n",
       "      <td>You're not dying.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38694</th>\n",
       "      <td>Вы не умираете.</td>\n",
       "      <td>You're not dying.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Source             Target\n",
       "38690  Ты не сумасшедшая.  You're not crazy.\n",
       "38691  Вы не сумасшедший.  You're not crazy.\n",
       "38692  Вы не сумасшедшая.  You're not crazy.\n",
       "38693     Ты не умираешь.  You're not dying.\n",
       "38694     Вы не умираете.  You're not dying."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d07c12e3-2ae9-4ff9-910d-157045ae2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_word = language_data['Source'].values\n",
    "target_word = language_data['Target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d4e6e5-6446-49c1-b5b3-2f06c0caf170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Марш!', 'Go.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_word[0], target_word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9becfba-23ea-4161-baf7-db337fff0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercasing the setences\n",
    "source_word_ = [x.lower() for x in source_word]\n",
    "target_word_ = [x.lower() for x in target_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a263f311-c1f2-4398-9cb7-25b657d87f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_word_ = [re.sub(\"'\",'',x) for x in source_word_]\n",
    "target_word_ = [re.sub(\"'\",'',x) for x in target_word_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "430f9bf7-974c-4c5c-9f08-20f196c87fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_word_ = [x.strip() for x in source_word_]\n",
    "target_word_ = [x.strip() for x in target_word_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "969fe0cb-c166-451b-8c9f-db502220db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_word_ = [re.sub(r\"[^\\w\\s]\", r\"\", x) for x in source_word_]\n",
    "target_word_ = [re.sub(r\"[^\\w\\s]\", r\"\", x) for x in target_word_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "380ba5df-9087-41c0-ba3b-213fc39b72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_word_ = [re.sub(r\"\\d\", r\"\", x) for x in source_word_]\n",
    "target_word_ = [re.sub(r\"\\d\", r\"\", x) for x in target_word_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48241391-0b5f-4894-a016-3abfa7b7b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_word_ = [re.sub('\"', '', x) for x in source_word_]\n",
    "target_word_ = [re.sub('\"', '', x) for x in target_word_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "561876ed-d15b-4d74-a6f0-ac977aa15200",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_word_ = [re.sub(r\"ӕ\", r\"æ\", x) for x in source_word_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b11ab261-579e-4796-9270-b7412d744761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_word_ = [f'{start_target} {x} {end_target}' for x in source_word_]\n",
    "target_word_ = [f'{start_target} {x} {end_target}' for x in target_word_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31e666cc-a2c4-44fa-b2be-5fce7a707e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('марш', 'sos go eos')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_word_[0], target_word_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66aacda-d7c5-432e-b4ef-26894828d82b",
   "metadata": {},
   "source": [
    "# Data spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cd1a947-3809-44e3-9a95-f7beb88e9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = source_word_\n",
    "Y = target_word_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f162257-a1b4-4f95-8924-c0a8d8cfec23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34825, 34825, 3870, 3870)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "len(X_train),len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "831ccdc4-2ac5-47a6-bdf8-03d08ecd2652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('марш', 'sos go eos')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], Y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a866d-44e2-47a3-8347-8f343cadafe6",
   "metadata": {},
   "source": [
    "## Data preparing for encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "979cfe58-63c5-4c4a-be81-23caf194cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for the word embedding\n",
    "def Max_length(data):\n",
    "    max_length_ = max([len(x.split(' ')) for x in data])\n",
    "    return max_length_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8a27121-00cd-48b0-93bb-d83113a79c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data\n",
    "max_lenght_source = Max_length(X_train)\n",
    "max_lenght_target = Max_length(y_train)\n",
    "\n",
    "#Test data\n",
    "max_lenght_source_test = Max_length(X_test)\n",
    "max_lenght_target_test = Max_length(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4af652ed-5759-4cce-8c21-1c54464057d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_lenght_target, max_lenght_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "830d97c3-5032-4a1d-9cc8-4a5e113bfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_(text_data):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text_data)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer_input = tokenizer_(X_train)\n",
    "vocab_size_input = len(tokenizer_input.word_index) + 1\n",
    "tokenizer_target = tokenizer_(y_train)\n",
    "vocab_size_target = len(tokenizer_target.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9c2df04-8720-4c4f-b2d9-8900f4bd7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{dir}-tokenizer_input.pkl','wb') as f:\n",
    "    pkl.dump(tokenizer_input, f)\n",
    "\n",
    "with open(f'{dir}-tokenizer_target.pkl','wb') as f:\n",
    "    pkl.dump(tokenizer_target, f)\n",
    "    \n",
    "pkl.dump(tokenizer_input, open(f'{dir}-tokenizer_input.pkl', 'wb'))\n",
    "pkl.dump(tokenizer_target, open(f'{dir}-tokenizer_target.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "403c63c9-7a66-403b-a79f-9e06fcee84e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11205, 4173)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_input, vocab_size_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81ad8a6a-e1dc-48de-b446-c2d7ff443ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_batch(X= X_train,Y=y_train, batch_size=128):\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_data_input = np.zeros((batch_size,max_lenght_source),dtype='float32') #metrix of batch_size*max_length_english\n",
    "            decoder_data_input = np.zeros((batch_size,max_lenght_target),dtype='float32') #metrix of batch_size*max_length_marathi\n",
    "            decoder_target_input = np.zeros((batch_size,max_lenght_target,vocab_size_target),dtype='float32') # 3d array one hot encoder decoder target data\n",
    "            for i, (input_text,target_text) in enumerate(zip(X[j:j+batch_size],Y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_data_input[i,t] = tokenizer_input.word_index[word] # Here we are storing the encoder \n",
    "                                                                         #seq in row here padding is done automaticaly as \n",
    "                                                                         #we have defined col as max_lenght\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    # if word == 'START_':\n",
    "                    #   word = 'start'\n",
    "                    # elif word == 'END_':\n",
    "                    #   word = 'end'\n",
    "                    decoder_data_input[i,t] = tokenizer_target.word_index[word] # same for the decoder sequence\n",
    "                    if t>0:\n",
    "                        decoder_target_input[i,t-1,tokenizer_target.word_index[word]] = 1 #target is one timestep ahead of decoder input because it does not have 'start tag'\n",
    "            # print(encoder_data_input.shape())\n",
    "            yield ([encoder_data_input,decoder_data_input],decoder_target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cecaf5a-26f8-4b12-a372-d88732504b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 50\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,),name=\"encoder_inputs\")\n",
    "emb_layer_encoder = Embedding(vocab_size_input,HIDDEN_DIM, mask_zero=True)(encoder_inputs)\n",
    "encoder = LSTM(HIDDEN_DIM, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(emb_layer_encoder)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,),name=\"decoder_inputs\")\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "emb_layer_decoder = Embedding(vocab_size_target,HIDDEN_DIM, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(HIDDEN_DIM, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(emb_layer_decoder, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_target, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63153d0e-941a-4bf0-87eb-ae29652c09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ebbda12-686d-47de-af07-63661bd10b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, to_file=f'{dir}-train_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d12c688a-25f0-426a-b9dd-873ee155dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 256\n",
    "epochs = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec1d51b8-bb57-4260-a64f-1e08364e1a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ONIGAT~1\\AppData\\Local\\Temp/ipykernel_11080/1782680856.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator = generator_batch(X_train, y_train, batch_size = batch_size), steps_per_epoch = train_samples//batch_size, epochs=epochs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 16s 76ms/step - loss: 3.3875 - accuracy: 0.1913\n",
      "Epoch 2/120\n",
      "136/136 [==============================] - 10s 77ms/step - loss: 2.8681 - accuracy: 0.2024\n",
      "Epoch 3/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 2.7094 - accuracy: 0.2303\n",
      "Epoch 4/120\n",
      "136/136 [==============================] - 11s 82ms/step - loss: 2.5802 - accuracy: 0.2398\n",
      "Epoch 5/120\n",
      "136/136 [==============================] - 11s 80ms/step - loss: 2.4793 - accuracy: 0.2580\n",
      "Epoch 6/120\n",
      "136/136 [==============================] - 11s 78ms/step - loss: 2.3945 - accuracy: 0.2692\n",
      "Epoch 7/120\n",
      "136/136 [==============================] - 11s 80ms/step - loss: 2.3149 - accuracy: 0.2776\n",
      "Epoch 8/120\n",
      "136/136 [==============================] - 11s 78ms/step - loss: 2.2377 - accuracy: 0.2855\n",
      "Epoch 9/120\n",
      "136/136 [==============================] - 11s 79ms/step - loss: 2.1637 - accuracy: 0.2942\n",
      "Epoch 10/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 2.0950 - accuracy: 0.3123\n",
      "Epoch 11/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 2.0296 - accuracy: 0.3314\n",
      "Epoch 12/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 1.9676 - accuracy: 0.3416\n",
      "Epoch 13/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 1.9086 - accuracy: 0.3512\n",
      "Epoch 14/120\n",
      "136/136 [==============================] - 11s 80ms/step - loss: 1.8539 - accuracy: 0.3600\n",
      "Epoch 15/120\n",
      "136/136 [==============================] - 10s 77ms/step - loss: 1.8034 - accuracy: 0.3685\n",
      "Epoch 16/120\n",
      "136/136 [==============================] - 10s 77ms/step - loss: 1.7557 - accuracy: 0.3786\n",
      "Epoch 17/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 1.7106 - accuracy: 0.3878\n",
      "Epoch 18/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 1.6684 - accuracy: 0.3960\n",
      "Epoch 19/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 1.6274 - accuracy: 0.4031\n",
      "Epoch 20/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 1.5878 - accuracy: 0.4098\n",
      "Epoch 21/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 1.5505 - accuracy: 0.4161\n",
      "Epoch 22/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 1.5145 - accuracy: 0.4222\n",
      "Epoch 23/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 1.4802 - accuracy: 0.4282\n",
      "Epoch 24/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 1.4469 - accuracy: 0.4338\n",
      "Epoch 25/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 1.4145 - accuracy: 0.4396\n",
      "Epoch 26/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 1.3827 - accuracy: 0.4453\n",
      "Epoch 27/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 1.3528 - accuracy: 0.4505\n",
      "Epoch 28/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 1.3234 - accuracy: 0.4562\n",
      "Epoch 29/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 1.2951 - accuracy: 0.4618\n",
      "Epoch 30/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 1.2680 - accuracy: 0.4669\n",
      "Epoch 31/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 1.2415 - accuracy: 0.4720\n",
      "Epoch 32/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 1.2165 - accuracy: 0.4771\n",
      "Epoch 33/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 1.1919 - accuracy: 0.4821\n",
      "Epoch 34/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 1.1681 - accuracy: 0.4872\n",
      "Epoch 35/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 1.1444 - accuracy: 0.4924\n",
      "Epoch 36/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 1.1228 - accuracy: 0.4971\n",
      "Epoch 37/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 1.1014 - accuracy: 0.5017\n",
      "Epoch 38/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 1.0798 - accuracy: 0.5068\n",
      "Epoch 39/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 1.0591 - accuracy: 0.5112\n",
      "Epoch 40/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 1.0395 - accuracy: 0.5155\n",
      "Epoch 41/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 1.0201 - accuracy: 0.5200\n",
      "Epoch 42/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 1.0015 - accuracy: 0.5243\n",
      "Epoch 43/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 0.9822 - accuracy: 0.5291\n",
      "Epoch 44/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 0.9645 - accuracy: 0.5333\n",
      "Epoch 45/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 0.9467 - accuracy: 0.5377\n",
      "Epoch 46/120\n",
      "136/136 [==============================] - 11s 80ms/step - loss: 0.9294 - accuracy: 0.5425\n",
      "Epoch 47/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.9122 - accuracy: 0.5470\n",
      "Epoch 48/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 0.8967 - accuracy: 0.5511\n",
      "Epoch 49/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 0.8811 - accuracy: 0.5551\n",
      "Epoch 50/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.8656 - accuracy: 0.5593\n",
      "Epoch 51/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 0.8502 - accuracy: 0.5629\n",
      "Epoch 52/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.8353 - accuracy: 0.5673\n",
      "Epoch 53/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 0.8213 - accuracy: 0.5703\n",
      "Epoch 54/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.8068 - accuracy: 0.5746\n",
      "Epoch 55/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.7933 - accuracy: 0.5780\n",
      "Epoch 56/120\n",
      "136/136 [==============================] - 10s 77ms/step - loss: 0.7802 - accuracy: 0.5813\n",
      "Epoch 57/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.7667 - accuracy: 0.5849\n",
      "Epoch 58/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.7534 - accuracy: 0.5887\n",
      "Epoch 59/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.7419 - accuracy: 0.5921\n",
      "Epoch 60/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.7298 - accuracy: 0.5949\n",
      "Epoch 61/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.7179 - accuracy: 0.5987\n",
      "Epoch 62/120\n",
      "136/136 [==============================] - 10s 77ms/step - loss: 0.7059 - accuracy: 0.6018\n",
      "Epoch 63/120\n",
      "136/136 [==============================] - 11s 77ms/step - loss: 0.6946 - accuracy: 0.6051\n",
      "Epoch 64/120\n",
      "136/136 [==============================] - 11s 77ms/step - loss: 0.6840 - accuracy: 0.6077\n",
      "Epoch 65/120\n",
      "136/136 [==============================] - 11s 77ms/step - loss: 0.6731 - accuracy: 0.6109\n",
      "Epoch 66/120\n",
      "136/136 [==============================] - 10s 77ms/step - loss: 0.6622 - accuracy: 0.6138\n",
      "Epoch 67/120\n",
      "136/136 [==============================] - 11s 77ms/step - loss: 0.6517 - accuracy: 0.6165\n",
      "Epoch 68/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.6421 - accuracy: 0.6188\n",
      "Epoch 69/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.6319 - accuracy: 0.6219\n",
      "Epoch 70/120\n",
      "136/136 [==============================] - 11s 77ms/step - loss: 0.6225 - accuracy: 0.6246\n",
      "Epoch 71/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 0.6132 - accuracy: 0.6273\n",
      "Epoch 72/120\n",
      "136/136 [==============================] - 10s 77ms/step - loss: 0.6031 - accuracy: 0.6301\n",
      "Epoch 73/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.5951 - accuracy: 0.6327\n",
      "Epoch 74/120\n",
      "136/136 [==============================] - 10s 71ms/step - loss: 0.5861 - accuracy: 0.6346\n",
      "Epoch 75/120\n",
      "136/136 [==============================] - 10s 71ms/step - loss: 0.5776 - accuracy: 0.6373\n",
      "Epoch 76/120\n",
      "136/136 [==============================] - 10s 72ms/step - loss: 0.5694 - accuracy: 0.6394\n",
      "Epoch 77/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.5614 - accuracy: 0.6419\n",
      "Epoch 78/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.5535 - accuracy: 0.6440\n",
      "Epoch 79/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.5461 - accuracy: 0.6459\n",
      "Epoch 80/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.5384 - accuracy: 0.6484\n",
      "Epoch 81/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.5308 - accuracy: 0.6503\n",
      "Epoch 82/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.5243 - accuracy: 0.6524\n",
      "Epoch 83/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.5170 - accuracy: 0.6541\n",
      "Epoch 84/120\n",
      "136/136 [==============================] - 10s 72ms/step - loss: 0.5106 - accuracy: 0.6558\n",
      "Epoch 85/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.5040 - accuracy: 0.6577\n",
      "Epoch 86/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4972 - accuracy: 0.6594\n",
      "Epoch 87/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4912 - accuracy: 0.6613\n",
      "Epoch 88/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4848 - accuracy: 0.6626\n",
      "Epoch 89/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4794 - accuracy: 0.6645\n",
      "Epoch 90/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4729 - accuracy: 0.6661\n",
      "Epoch 91/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4671 - accuracy: 0.6674\n",
      "Epoch 92/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4621 - accuracy: 0.6693\n",
      "Epoch 93/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4567 - accuracy: 0.6704\n",
      "Epoch 94/120\n",
      "136/136 [==============================] - 10s 76ms/step - loss: 0.4512 - accuracy: 0.6720\n",
      "Epoch 95/120\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 0.4455 - accuracy: 0.6737\n",
      "Epoch 96/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 0.4398 - accuracy: 0.6751\n",
      "Epoch 97/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4362 - accuracy: 0.6764\n",
      "Epoch 98/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4308 - accuracy: 0.6777\n",
      "Epoch 99/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4262 - accuracy: 0.6793\n",
      "Epoch 100/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4216 - accuracy: 0.6803\n",
      "Epoch 101/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 0.4176 - accuracy: 0.6816\n",
      "Epoch 102/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4130 - accuracy: 0.6825\n",
      "Epoch 103/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4091 - accuracy: 0.6839\n",
      "Epoch 104/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4044 - accuracy: 0.6847\n",
      "Epoch 105/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.4006 - accuracy: 0.6862\n",
      "Epoch 106/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.3964 - accuracy: 0.6871\n",
      "Epoch 107/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.3929 - accuracy: 0.6883\n",
      "Epoch 108/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.3891 - accuracy: 0.6892\n",
      "Epoch 109/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.3850 - accuracy: 0.6904\n",
      "Epoch 110/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.3814 - accuracy: 0.6912\n",
      "Epoch 111/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.3784 - accuracy: 0.6921\n",
      "Epoch 112/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.3742 - accuracy: 0.6934\n",
      "Epoch 113/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.3716 - accuracy: 0.6942\n",
      "Epoch 114/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 0.3678 - accuracy: 0.6950\n",
      "Epoch 115/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.3652 - accuracy: 0.6957\n",
      "Epoch 116/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.3623 - accuracy: 0.6967\n",
      "Epoch 117/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.3588 - accuracy: 0.6971\n",
      "Epoch 118/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.3557 - accuracy: 0.6985\n",
      "Epoch 119/120\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.3531 - accuracy: 0.6991\n",
      "Epoch 120/120\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 0.3502 - accuracy: 0.6999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e656cf0040>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%capture\n",
    "model.fit_generator(generator = generator_batch(X_train, y_train, batch_size = batch_size), steps_per_epoch = train_samples//batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7db8682d-539a-4ace-9c95-3f414ee4fb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(f'{dir}-model-{epochs}.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(f'{dir}-model-{epochs}_weight.h5')\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e412cb7-8fb6-4910-85e2-8c4ea3f10c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(f'{dir}-model-{epochs}.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_loaded = model_from_json(loaded_model_json)\n",
    "\n",
    "model_loaded.load_weights(f'{dir}-model-{epochs}_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8ac9691-08ef-462c-ba5a-255d2d9bb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs_inf = model_loaded.input[0]\n",
    "encoder_outputs_inf, inf_state_h, inf_state_c = model_loaded.layers[4].output\n",
    "encoder_inf_states = [inf_state_h,inf_state_c]\n",
    "encoder_model = Model(encoder_inputs_inf,encoder_inf_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb2d5383-b268-427f-abca-67bcdbd59614",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_h_input = Input(shape=(HIDDEN_DIM,))\n",
    "decoder_state_c_input = Input(shape=(HIDDEN_DIM,))\n",
    "decoder_state_input = [decoder_state_h_input,decoder_state_c_input]\n",
    "\n",
    "decoder_input_inf = model_loaded.input[1]\n",
    "decoder_emb_inf = model_loaded.layers[3](decoder_input_inf)\n",
    "decoder_lstm_inf = model_loaded.layers[5]\n",
    "decoder_output_inf, decoder_state_h_inf, decoder_state_c_inf = decoder_lstm_inf(decoder_emb_inf, initial_state =decoder_state_input)\n",
    "decoder_state_inf = [decoder_state_h_inf,decoder_state_c_inf]\n",
    "dense_inf = model_loaded.layers[6]\n",
    "decoder_output_final = dense_inf(decoder_output_inf)\n",
    "\n",
    "decoder_model = Model([decoder_input_inf]+decoder_state_input,[decoder_output_final]+decoder_state_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82b5c70c-a9ef-4da6-a1d5-fe34fe94238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{dir}-tokenizer_input.pkl','rb') as f:\n",
    "    tokenizer_input = pkl.load(f)\n",
    "with open(f'{dir}-tokenizer_target.pkl','rb') as f:\n",
    "    tokenizer_target = pkl.load(f)\n",
    "\n",
    "reverse_word_map_input = dict(map(reversed, tokenizer_input.word_index.items()))\n",
    "reverse_word_map_target = dict(map(reversed, tokenizer_target.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8714b9ba-6a9e-4b55-a8af-60f8ca3da9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(input_seq):\n",
    "    state_values_encoder = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tokenizer_target.word_index[start_target]\n",
    "    stop_condition = False\n",
    "    decoder_sentance = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        sample_word , decoder_h,decoder_c= decoder_model.predict([target_seq] + state_values_encoder)\n",
    "        sample_word_index = np.argmax(sample_word[0,-1,:])\n",
    "        decoder_word = reverse_word_map_target[sample_word_index]\n",
    "        decoder_sentance += ' '+ decoder_word\n",
    "        if (decoder_word == end_target or \n",
    "            len(decoder_sentance) > 70):\n",
    "            stop_condition = True\n",
    "        target_seq[0, 0] = sample_word_index\n",
    "        state_values_encoder = [decoder_h,decoder_c]\n",
    "    return decoder_sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f03d272-1854-4bb8-8c55-d48a42a2e103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentance:  1\n",
      "sentance:  как ты можешь помочь\n",
      "origianl translate:  how can you help \n",
      "predicted Translate:  you isnt pain you \n",
      "====================================================================================================\n",
      "Test sentance:  2\n",
      "sentance:  ты напугал нас\n",
      "origianl translate:  you scared us \n",
      "predicted Translate:  you friends me \n",
      "====================================================================================================\n",
      "Test sentance:  3\n",
      "sentance:  я не нервничал\n",
      "origianl translate:  i wasnt nervous \n",
      "predicted Translate:  im ill expert \n",
      "====================================================================================================\n",
      "Test sentance:  4\n",
      "sentance:  я сказал сиди здесь\n",
      "origianl translate:  i said stay here \n",
      "predicted Translate:  i hurry smell it \n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    sentance = X_test[i]\n",
    "    original_target = y_test[i]\n",
    "    input_seq = tokenizer_input.texts_to_sequences([sentance])\n",
    "    pad_sequence = pad_sequences(input_seq, maxlen= 30, padding='post')\n",
    "    predicted_target = decode_seq(pad_sequence)\n",
    "    print(\"Test sentance: \",i+1)\n",
    "    print(\"sentance: \",sentance)\n",
    "    print(\"origianl translate:\",original_target[3:-3])\n",
    "    print(\"predicted Translate:\",predicted_target[:-3])\n",
    "    print(\"==\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5286e0b3-0c4c-48d5-b817-40e1339df1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentance:  1\n",
      "sentance:  том встал\n",
      "origianl translate:  tom got up \n",
      "predicted Translate:  tom is mary \n",
      "====================================================================================================\n",
      "Test sentance:  2\n",
      "sentance:  куришь\n",
      "origianl translate:  do you smoke \n",
      "predicted Translate:  towel spoon \n",
      "====================================================================================================\n",
      "Test sentance:  3\n",
      "sentance:  я свою часть сделал\n",
      "origianl translate:  i did my part \n",
      "predicted Translate:  i leave it \n",
      "====================================================================================================\n",
      "Test sentance:  4\n",
      "sentance:  у вас есть дети\n",
      "origianl translate:  do you have kids \n",
      "predicted Translate:  do have it has \n",
      "====================================================================================================\n",
      "Test sentance:  5\n",
      "sentance:  тому нас видно\n",
      "origianl translate:  can tom see us \n",
      "predicted Translate:  was tom won \n",
      "====================================================================================================\n",
      "Test sentance:  6\n",
      "sentance:  это так глупо\n",
      "origianl translate:  thats so silly \n",
      "predicted Translate:  it was comics \n",
      "====================================================================================================\n",
      "Test sentance:  7\n",
      "sentance:  прекрати спорить\n",
      "origianl translate:  stop arguing \n",
      "predicted Translate:  got armed \n",
      "====================================================================================================\n",
      "Test sentance:  8\n",
      "sentance:  никто не погиб\n",
      "origianl translate:  no one is dead \n",
      "predicted Translate:  lot is turn \n",
      "====================================================================================================\n",
      "Test sentance:  9\n",
      "sentance:  ты ноль\n",
      "origianl translate:  youre nothing \n",
      "predicted Translate:  youre a hailing \n",
      "====================================================================================================\n",
      "Test sentance:  10\n",
      "sentance:  это не ко мне\n",
      "origianl translate:  dont ask me \n",
      "predicted Translate:  i hes wait \n",
      "====================================================================================================\n",
      "Test sentance:  11\n",
      "sentance:  ты не певица\n",
      "origianl translate:  youre no singer \n",
      "predicted Translate:  you wont won \n",
      "====================================================================================================\n",
      "Test sentance:  12\n",
      "sentance:  ты не уверен\n",
      "origianl translate:  arent you sure \n",
      "predicted Translate:  do you ill theyre \n",
      "====================================================================================================\n",
      "Test sentance:  13\n",
      "sentance:  кто сделал вклад\n",
      "origianl translate:  who contributed \n",
      "predicted Translate:  you one mouth \n",
      "====================================================================================================\n",
      "Test sentance:  14\n",
      "sentance:  том меня не увидит\n",
      "origianl translate:  tom wont see me \n",
      "predicted Translate:  tom hes again \n",
      "====================================================================================================\n",
      "Test sentance:  15\n",
      "sentance:  том в опасности\n",
      "origianl translate:  is tom in danger \n",
      "predicted Translate:  tom hate tom \n",
      "====================================================================================================\n",
      "Test sentance:  16\n",
      "sentance:  ты состоятельный\n",
      "origianl translate:  youre wealthy \n",
      "predicted Translate:  he you dream \n",
      "====================================================================================================\n",
      "Test sentance:  17\n",
      "sentance:  я хочу молока\n",
      "origianl translate:  i want some milk \n",
      "predicted Translate:  i needed me \n",
      "====================================================================================================\n",
      "Test sentance:  18\n",
      "sentance:  ты не заплатил\n",
      "origianl translate:  you didnt pay \n",
      "predicted Translate:  bad insulted \n",
      "====================================================================================================\n",
      "Test sentance:  19\n",
      "sentance:  у меня воспаление лёгких\n",
      "origianl translate:  i have pneumonia \n",
      "predicted Translate:  im on unprepared let \n",
      "====================================================================================================\n",
      "Test sentance:  20\n",
      "sentance:  вы поели\n",
      "origianl translate:  have you eaten \n",
      "predicted Translate:  can you kind \n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    sentance = X_train[i]\n",
    "    original_target = y_train[i]\n",
    "    input_seq = tokenizer_input.texts_to_sequences([sentance])\n",
    "    pad_sequence = pad_sequences(input_seq, maxlen= 30, padding='post')\n",
    "    predicted_target = decode_seq(pad_sequence)\n",
    "    print(\"Test sentance: \",i+1)\n",
    "    print(\"sentance: \",sentance)\n",
    "    print(\"origianl translate:\",original_target[3:-3])\n",
    "    print(\"predicted Translate:\",predicted_target[:-3])\n",
    "    print(\"==\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7dc63-781f-4ea5-b1b4-a702d7fe8b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentance = str(input())\n",
    "# input_seq = tokenizer_input.texts_to_sequences([sentance])\n",
    "# pad_sequence = pad_sequences(input_seq, maxlen= 30, padding='post')\n",
    "# predicted_target = decode_seq(pad_sequence)\n",
    "# print(\"predicted Translate:\",predicted_target[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c15b6-9095-453e-abb7-465a7504273a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
