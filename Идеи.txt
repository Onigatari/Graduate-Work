1. Преобразование слов в вектора с помощью One-Hot Encoder для задач NLP.
	Не самая лучшая идея так-как сложно задать метрику, 
	ведь все полученные вектора артоганальны
2. Context embedding - для представление слов 
	(см Лекция. Введение в NLP. Эмбеддинги слов (06.03.2020) Img - 1)
	Теряет смысл для редких слов. Потому что редко встречающиеся слова 
	будут пустыми.
3. При большой матрице можно испльзоовать PCA - это проекция обьекта из большего 
	ЛП в ЛП меньшей размерности (https://habr.com/ru/post/304214/)
4. Latent semantic analysis - Строки это документы нами изучаемые, столбцы это 
	слова. На их пересичении стоит кол-во встречь в документи слова x_i
	(см Лекция. Введение в NLP. Эмбеддинги слов (06.03.2020) Img - 2)
5. Используя SVD разложение можно получить 3 матрицы благодаря, которым можно понять
	насколько близко по смыслу стоит то или ино слово (Косинусное растояние)
6. Tf-idf - Матрица как в SVD, но хрен знает как работает.
	(см Лекция. Введение в NLP. Эмбеддинги слов (06.03.2020) Img - 3)
7. Word2Vec - Эмбеддинг слов, работает через SoftMax models 
	(https://ru.wikipedia.org/wiki/Softmax)
	(!!!Почитай, что это такое. Не прочитаешь - не осетин!!!)