GitHub: https://github.com/bentrevett/pytorch-seq2seq
Визуализация Seq2seq: https://habr.com/ru/post/486158/
Mini Seq2seq: https://github.com/keon/seq2seq
История развития я машинного перевода: https://vc.ru/future/32616-mashinnyy-perevod-ot-holodnoy-voyny-do-glubokogo-obucheniya
Лекция Seq2Seq: https://www.youtube.com/watch?v=N3TLYsn0TU8
Лекция Внимание (Attention): https://www.youtube.com/watch?v=G4vT5cvJSxY
Seq2seq на PyTorch: https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb
Оригинальная статься: https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf
https://tatoeba.org/ru/downloads
https://github.com/vasiliyeskin/bentrevett-pytorch-seq2seq_ru/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb

Лекция. Введение в NLP. Эмбеддинги слов (06.03.2020)
https://www.youtube.com/watch?v=CWpe4uD9vr4&t

1. Преобразование слов в вектора с помощью One-Hot Encoder для задач NLP.
	Не самая лучшая идея так-как сложно задать метрику, 
	ведь все полученные вектора ортогональны
2. Context embedding - для представление слов 
	(см Лекция. Введение в NLP. Эмбеддинги слов (06.03.2020) Img - 1)
	Теряет смысл для редких слов. Потому что редко встречающиеся слова 
	будут пустыми.
3. При большой матрице можно использовать PCA - это проекция объекта из большего 
	ЛП в ЛП меньшей размерности (https://habr.com/ru/post/304214/)
4. Latent semantic analysis - Строки это документы нами изучаемые, столбцы это 
	слова. На их пересечении стоит кол-во встреч в документе слова x_i
	(см Лекция. Введение в NLP. Эмбеддинги слов (06.03.2020) Img - 2)
5. Используя SVD разложение можно получить 3 матрицы благодаря, которым можно понять
	насколько близко по смыслу стоит то или но слово (Косинусное расстояние)
6. Tf-idf - Матрица как в SVD, но хрен знает как работает.
	(см Лекция. Введение в NLP. Эмбеддинги слов (06.03.2020) Img - 3)
7. Word2Vec - Эмбеддинг слов, работает через SoftMax models 
	(https://ru.wikipedia.org/wiki/Softmax)
	(!!!Почитай, что это такое. Не прочитаешь - не осетин!!!)
8. Прикол с результатом Word2Vec
	(см Лекция. Введение в NLP. Эмбеддинги слов (06.03.2020) Img - 4)

Лекция. Seq2seq & Attention (27.03.2020)
https://www.youtube.com/watch?v=rZC2_BKRUAk&t

1. Для корректности машинного перевода нужно знать как минимум контекст предложения, 
	а лучше контекст всего переводимого текста. Для этого хорошо подходит Encoder-Decoder
2. Проблемы Encoder-Decoder: Забывчивость контекста больших предложений, мы выбираем слово по максимальной вероятности - 
	это не всегда хорошо.
3. Bidirectional RNN - два слоя в Encoder одна идет слева направо (->), другая с права налево (<-). Это поможет решить
	проблему забывчивости контекста.
4. Beam search - решает проблему, проблему выбора максимальной вероятностям. Строятся несколько последовательностей по максимальным
	вероятностям. В среднем 4-8. 
5. Teacher-forcing - вместе с тем что моя модель выдавала я передаю в следующий этап перевод который должен был быть.

Map для осетинского
BATCH
LSTM