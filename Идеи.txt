Лекция. Введение в NLP. (14.02.2021)
https://www.youtube.com/watch?v=d0oV1MZ_KdE

1. Одна из проблем моего переводчика, в том что в словаре, который используется
	при генерации перевода находятся слова одинаковой смысловой нагрузки.
	Не происходит нармолизация слов. Это сильно Ухудшает качество перевода.
	Однако, после того как мы нормализуем слова нам нужен механиз для генерации
	перевода с правильной подстаеновкой форм слов.

Лекция. Введение в NLP. Эмбеддинги слов (06.03.2020)
https://www.youtube.com/watch?v=CWpe4uD9vr4&t

1. Преобразование слов в вектора с помощью One-Hot Encoder для задач NLP.
	Не самая лучшая идея так-как сложно задать метрику, 
	ведь все полученные вектора ортогональны
2. Context embedding - для представление слов 
	(см Лекция. Введение в NLP. Эмбеддинги слов (06.03.2020) Img - 1)
	Теряет смысл для редких слов. Потому что редко встречающиеся слова 
	будут пустыми.
3. При большой матрице можно использовать PCA - это проекция объекта из большего 
	ЛП в ЛП меньшей размерности (https://habr.com/ru/post/304214/)
4. Latent semantic analysis - Строки это документы нами изучаемые, столбцы это 
	слова. На их пересечении стоит кол-во встреч в документе слова x_i
	(см Лекция. Введение в NLP. Эмбеддинги слов (06.03.2020) Img - 2)
5. Используя SVD разложение можно получить 3 матрицы благодаря, которым можно понять
	насколько близко по смыслу стоит то или но слово (Косинусное расстояние)
6. Tf-idf - Матрица как в SVD, но хрен знает как работает.
	(см Лекция. Введение в NLP. Эмбеддинги слов (06.03.2020) Img - 3)
7. Word2Vec - Эмбеддинг слов, работает через SoftMax models 
	(https://ru.wikipedia.org/wiki/Softmax)
	(!!!Почитай, что это такое. Не прочитаешь - не осетин!!!)
8. Прикол с результатом Word2Vec
	(см Лекция. Введение в NLP. Эмбеддинги слов (06.03.2020) Img - 4)

Лекция. Seq2seq & Attention (27.03.2020)
https://www.youtube.com/watch?v=rZC2_BKRUAk&t

1. Для корректности машинного перевода нужно знать как минимум контекст предложения, 
	а лучше контекст всего переводимого текста. Для этого хорошо подходит Encoder-Decoder
2. Проблемы Encoder-Decoder: Забывчивость контекста больших предложений, мы выбираем слово по максимальной вероятности - 
	это не всегда хорошо.
3. Bidirectional RNN - два слоя в Encoder одна идет слева направо (->), другая с права налево (<-). Это поможет решить
	проблему забывчивости контекста.
4. Beam search - решает проблему, проблему выбора максимальной вероятностям. Строятся несколько последовательностей по максимальным
	вероятностям. В среднем 4-8. 
5. Teacher-forcing - вместе с тем что моя модель выдавала я передаю в следующий этап перевод который должен был быть.